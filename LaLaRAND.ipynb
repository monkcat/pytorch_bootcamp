{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e934a126-fcdc-440a-ad0a-d43ea9cca79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
    "# or\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', pretrained=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d44fd703-14ff-43b1-880d-33f492882f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SqueezeNet                               --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       14,208\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─MaxPool2d: 2-3                    --\n",
       "│    └─Fire: 2-4                         --\n",
       "│    │    └─Conv2d: 3-1                  1,552\n",
       "│    │    └─ReLU: 3-2                    --\n",
       "│    │    └─Conv2d: 3-3                  1,088\n",
       "│    │    └─ReLU: 3-4                    --\n",
       "│    │    └─Conv2d: 3-5                  9,280\n",
       "│    │    └─ReLU: 3-6                    --\n",
       "│    └─Fire: 2-5                         --\n",
       "│    │    └─Conv2d: 3-7                  2,064\n",
       "│    │    └─ReLU: 3-8                    --\n",
       "│    │    └─Conv2d: 3-9                  1,088\n",
       "│    │    └─ReLU: 3-10                   --\n",
       "│    │    └─Conv2d: 3-11                 9,280\n",
       "│    │    └─ReLU: 3-12                   --\n",
       "│    └─Fire: 2-6                         --\n",
       "│    │    └─Conv2d: 3-13                 4,128\n",
       "│    │    └─ReLU: 3-14                   --\n",
       "│    │    └─Conv2d: 3-15                 4,224\n",
       "│    │    └─ReLU: 3-16                   --\n",
       "│    │    └─Conv2d: 3-17                 36,992\n",
       "│    │    └─ReLU: 3-18                   --\n",
       "│    └─MaxPool2d: 2-7                    --\n",
       "│    └─Fire: 2-8                         --\n",
       "│    │    └─Conv2d: 3-19                 8,224\n",
       "│    │    └─ReLU: 3-20                   --\n",
       "│    │    └─Conv2d: 3-21                 4,224\n",
       "│    │    └─ReLU: 3-22                   --\n",
       "│    │    └─Conv2d: 3-23                 36,992\n",
       "│    │    └─ReLU: 3-24                   --\n",
       "│    └─Fire: 2-9                         --\n",
       "│    │    └─Conv2d: 3-25                 12,336\n",
       "│    │    └─ReLU: 3-26                   --\n",
       "│    │    └─Conv2d: 3-27                 9,408\n",
       "│    │    └─ReLU: 3-28                   --\n",
       "│    │    └─Conv2d: 3-29                 83,136\n",
       "│    │    └─ReLU: 3-30                   --\n",
       "│    └─Fire: 2-10                        --\n",
       "│    │    └─Conv2d: 3-31                 18,480\n",
       "│    │    └─ReLU: 3-32                   --\n",
       "│    │    └─Conv2d: 3-33                 9,408\n",
       "│    │    └─ReLU: 3-34                   --\n",
       "│    │    └─Conv2d: 3-35                 83,136\n",
       "│    │    └─ReLU: 3-36                   --\n",
       "│    └─Fire: 2-11                        --\n",
       "│    │    └─Conv2d: 3-37                 24,640\n",
       "│    │    └─ReLU: 3-38                   --\n",
       "│    │    └─Conv2d: 3-39                 16,640\n",
       "│    │    └─ReLU: 3-40                   --\n",
       "│    │    └─Conv2d: 3-41                 147,712\n",
       "│    │    └─ReLU: 3-42                   --\n",
       "│    └─MaxPool2d: 2-12                   --\n",
       "│    └─Fire: 2-13                        --\n",
       "│    │    └─Conv2d: 3-43                 32,832\n",
       "│    │    └─ReLU: 3-44                   --\n",
       "│    │    └─Conv2d: 3-45                 16,640\n",
       "│    │    └─ReLU: 3-46                   --\n",
       "│    │    └─Conv2d: 3-47                 147,712\n",
       "│    │    └─ReLU: 3-48                   --\n",
       "├─Sequential: 1-2                        --\n",
       "│    └─Dropout: 2-14                     --\n",
       "│    └─Conv2d: 2-15                      513,000\n",
       "│    └─ReLU: 2-16                        --\n",
       "│    └─AdaptiveAvgPool2d: 2-17           --\n",
       "=================================================================\n",
       "Total params: 1,248,424\n",
       "Trainable params: 1,248,424\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7a2f473-5c49-4086-b82e-e40fd797ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0 : Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "features.1 : ReLU(inplace=True)\n",
      "features.2 : MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "features.3.squeeze : Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.3.squeeze_activation : ReLU(inplace=True)\n",
      "features.3.expand1x1 : Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.3.expand1x1_activation : ReLU(inplace=True)\n",
      "features.3.expand3x3 : Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.3.expand3x3_activation : ReLU(inplace=True)\n",
      "features.4.squeeze : Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.4.squeeze_activation : ReLU(inplace=True)\n",
      "features.4.expand1x1 : Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.4.expand1x1_activation : ReLU(inplace=True)\n",
      "features.4.expand3x3 : Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.4.expand3x3_activation : ReLU(inplace=True)\n",
      "features.5.squeeze : Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.5.squeeze_activation : ReLU(inplace=True)\n",
      "features.5.expand1x1 : Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.5.expand1x1_activation : ReLU(inplace=True)\n",
      "features.5.expand3x3 : Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.5.expand3x3_activation : ReLU(inplace=True)\n",
      "features.6 : MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "features.7.squeeze : Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.7.squeeze_activation : ReLU(inplace=True)\n",
      "features.7.expand1x1 : Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.7.expand1x1_activation : ReLU(inplace=True)\n",
      "features.7.expand3x3 : Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.7.expand3x3_activation : ReLU(inplace=True)\n",
      "features.8.squeeze : Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.8.squeeze_activation : ReLU(inplace=True)\n",
      "features.8.expand1x1 : Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.8.expand1x1_activation : ReLU(inplace=True)\n",
      "features.8.expand3x3 : Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.8.expand3x3_activation : ReLU(inplace=True)\n",
      "features.9.squeeze : Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.9.squeeze_activation : ReLU(inplace=True)\n",
      "features.9.expand1x1 : Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.9.expand1x1_activation : ReLU(inplace=True)\n",
      "features.9.expand3x3 : Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.9.expand3x3_activation : ReLU(inplace=True)\n",
      "features.10.squeeze : Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.10.squeeze_activation : ReLU(inplace=True)\n",
      "features.10.expand1x1 : Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.10.expand1x1_activation : ReLU(inplace=True)\n",
      "features.10.expand3x3 : Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.10.expand3x3_activation : ReLU(inplace=True)\n",
      "features.11 : MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "features.12.squeeze : Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.12.squeeze_activation : ReLU(inplace=True)\n",
      "features.12.expand1x1 : Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.12.expand1x1_activation : ReLU(inplace=True)\n",
      "features.12.expand3x3 : Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.12.expand3x3_activation : ReLU(inplace=True)\n",
      "classifier.0 : Dropout(p=0.5, inplace=False)\n",
      "classifier.1 : Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
      "classifier.2 : ReLU(inplace=True)\n",
      "classifier.3 : AdaptiveAvgPool2d(output_size=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "def get_leaf_named_modules(module, prefix=''):\n",
    "    leaves = []\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "\n",
    "        if len(list(child.children())) == 0:\n",
    "            leaves.append((full_name, child))\n",
    "        else:\n",
    "            leaves.extend(get_leaf_named_modules(child, prefix=full_name))\n",
    "    \n",
    "    return leaves\n",
    "\n",
    "leaves = get_leaf_named_modules(model)\n",
    "for name, mod in leaves:\n",
    "    print(name, \":\", mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b6b6da4-ac2b-48e5-946d-ad78476843f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Target layer: Dropout(p=0.5, inplace=False)\n",
      "Hooks registered successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# GPU가 사용 가능한지 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 특정 레이어를 CPU로 이동시키는 hook 함수\n",
    "def cpu_hook(module, input, output):\n",
    "    \"\"\"\n",
    "    Forward hook: 이 레이어의 출력을 CPU로 이동\n",
    "    \"\"\"\n",
    "    print(f\"Moving output of {module.__class__.__name__} to CPU\")\n",
    "    return output.cpu()\n",
    "\n",
    "def input_cpu_hook(module, input):\n",
    "    \"\"\"\n",
    "    Forward pre-hook: 이 레이어의 입력을 CPU로 이동\n",
    "    \"\"\"\n",
    "    print(f\"Moving input of {module.__class__.__name__} to CPU\")\n",
    "    if isinstance(input, tuple):\n",
    "        return tuple(inp.cpu() if torch.is_tensor(inp) else inp for inp in input)\n",
    "    else:\n",
    "        return input.cpu() if torch.is_tensor(input) else input\n",
    "\n",
    "# 예제: classifier의 첫 번째 Dropout 레이어를 CPU에서 연산하도록 설정\n",
    "target_layer = model.classifier[0]  # Dropout 레이어\n",
    "print(f\"Target layer: {target_layer}\")\n",
    "\n",
    "# Pre-hook 등록 (입력을 CPU로 이동)\n",
    "pre_hook_handle = target_layer.register_forward_pre_hook(input_cpu_hook)\n",
    "\n",
    "# Post-hook 등록 (출력을 다시 원래 device로 이동)\n",
    "def output_to_device_hook(module, input, output):\n",
    "    \"\"\"출력을 다시 GPU로 이동 (다음 레이어를 위해)\"\"\"\n",
    "    print(f\"Moving output of {module.__class__.__name__} back to {device}\")\n",
    "    return output.to(device)\n",
    "\n",
    "post_hook_handle = target_layer.register_forward_hook(output_to_device_hook)\n",
    "\n",
    "print(\"Hooks registered successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4002460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3, 224, 224]), Input device: cuda:0\n",
      "Moving input of Dropout to CPU\n",
      "Moving output of Dropout back to cuda\n",
      "Output shape: torch.Size([2, 1000]), Output device: cuda:0\n",
      "\n",
      "Hook execution completed!\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 더미 입력 생성\n",
    "batch_size = 2\n",
    "test_input = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "print(f\"Input shape: {test_input.shape}, Input device: {test_input.device}\")\n",
    "\n",
    "# 모델 실행 (hook이 동작하는 것을 확인)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "    print(f\"Output shape: {output.shape}, Output device: {output.device}\")\n",
    "\n",
    "print(\"\\nHook execution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ddc434d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original first layer: Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "Wrapped layer: CPULayerWrapper(\n",
      "  (layer): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      ")\n",
      "Input device: cuda:0\n",
      "Moving input of Dropout to CPU\n",
      "Moving output of Dropout back to cuda\n",
      "Output device: cuda:0\n",
      "CPU layer execution completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# 더 고급 예제: 특정 레이어를 완전히 CPU에서 연산하는 클래스\n",
    "class CPULayerWrapper(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer.cpu()  # 레이어를 CPU에 유지\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 입력을 CPU로 이동\n",
    "        x_cpu = x.cpu()\n",
    "        \n",
    "        # CPU에서 연산 수행\n",
    "        output = self.layer(x_cpu)\n",
    "        \n",
    "        # 출력을 원래 device로 이동\n",
    "        return output.to(x.device)\n",
    "\n",
    "# 예제: features의 첫 번째 Conv2d 레이어를 CPU에서 연산하도록 교체\n",
    "print(\"Original first layer:\", model.features[0])\n",
    "\n",
    "# 원본 레이어를 CPU 래퍼로 교체\n",
    "original_layer = model.features[0]\n",
    "model.features[0] = CPULayerWrapper(original_layer)\n",
    "\n",
    "print(\"Wrapped layer:\", model.features[0])\n",
    "\n",
    "# 테스트\n",
    "test_input2 = torch.randn(1, 3, 224, 224).to(device)\n",
    "print(f\"Input device: {test_input2.device}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output2 = model(test_input2)\n",
    "    print(f\"Output device: {output2.device}\")\n",
    "    print(\"CPU layer execution completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92f2b994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing hooks...\n",
      "Hooks removed successfully!\n",
      "\n",
      "=== Memory Usage Comparison ===\n",
      "Before CPU layer execution:\n",
      "GPU memory allocated: 22.83 MB\n",
      "GPU memory cached: 72.00 MB\n",
      "\n",
      "After CPU layer execution:\n",
      "GPU memory allocated: 22.83 MB\n",
      "GPU memory cached: 72.00 MB\n",
      "\n",
      "=== Hook Usage Summary ===\n",
      "1. register_forward_pre_hook(): 레이어 실행 전에 호출\n",
      "2. register_forward_hook(): 레이어 실행 후에 호출\n",
      "3. register_backward_hook(): 역전파 시에 호출\n",
      "4. Hook을 사용하면 GPU ↔ CPU 간 데이터 이동으로 성능 저하 가능\n",
      "5. 메모리 절약이 목적이라면 gradient checkpointing을 고려해보세요\n"
     ]
    }
   ],
   "source": [
    "# Hook 제거 및 정리\n",
    "print(\"Removing hooks...\")\n",
    "if 'pre_hook_handle' in locals():\n",
    "    pre_hook_handle.remove()\n",
    "if 'post_hook_handle' in locals():\n",
    "    post_hook_handle.remove()\n",
    "print(\"Hooks removed successfully!\")\n",
    "\n",
    "# 메모리 사용량 비교 예제\n",
    "def check_memory_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n=== Memory Usage Comparison ===\")\n",
    "print(\"Before CPU layer execution:\")\n",
    "check_memory_usage()\n",
    "\n",
    "# CPU에서 연산하는 레이어가 있는 상태에서 실행\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        test_batch = torch.randn(4, 3, 224, 224).to(device)\n",
    "        _ = model(test_batch)\n",
    "\n",
    "print(\"\\nAfter CPU layer execution:\")\n",
    "check_memory_usage()\n",
    "\n",
    "print(\"\\n=== Hook Usage Summary ===\")\n",
    "print(\"1. register_forward_pre_hook(): 레이어 실행 전에 호출\")\n",
    "print(\"2. register_forward_hook(): 레이어 실행 후에 호출\")\n",
    "print(\"3. register_backward_hook(): 역전파 시에 호출\")\n",
    "print(\"4. Hook을 사용하면 GPU ↔ CPU 간 데이터 이동으로 성능 저하 가능\")\n",
    "print(\"5. 메모리 절약이 목적이라면 gradient checkpointing을 고려해보세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a95f3acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LayerDeviceController...\n",
      "Total leaf layers in model: 57\n",
      "\n",
      "Leaf layers (57):\n",
      "[00] features.0.layer: Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
      "[01] features.1: ReLU(inplace=True)\n",
      "[02] features.2: MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "[03] features.3.squeeze: Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "[04] features.3.squeeze_activation: ReLU(inplace=True)\n",
      "[05] features.3.expand1x1: Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "[06] features.3.expand1x1_activation: ReLU(inplace=True)\n",
      "[07] features.3.expand3x3: Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[08] features.3.expand3x3_activation: ReLU(inplace=True)\n",
      "[09] features.4.squeeze: Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "[10] features.4.squeeze_activation: ReLU(inplace=True)\n",
      "[11] features.4.expand1x1: Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "[12] features.4.expand1x1_activation: ReLU(inplace=True)\n",
      "[13] features.4.expand3x3: Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[14] features.4.expand3x3_activation: ReLU(inplace=True)\n",
      "[15] features.5.squeeze: Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "[16] features.5.squeeze_activation: ReLU(inplace=True)\n",
      "[17] features.5.expand1x1: Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "[18] features.5.expand1x1_activation: ReLU(inplace=True)\n",
      "[19] features.5.expand3x3: Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[20] features.5.expand3x3_activation: ReLU(inplace=True)\n",
      "[21] features.6: MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "[22] features.7.squeeze: Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "[23] features.7.squeeze_activation: ReLU(inplace=True)\n",
      "[24] features.7.expand1x1: Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "[25] features.7.expand1x1_activation: ReLU(inplace=True)\n",
      "[26] features.7.expand3x3: Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[27] features.7.expand3x3_activation: ReLU(inplace=True)\n",
      "[28] features.8.squeeze: Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "[29] features.8.squeeze_activation: ReLU(inplace=True)\n",
      "[30] features.8.expand1x1: Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "[31] features.8.expand1x1_activation: ReLU(inplace=True)\n",
      "[32] features.8.expand3x3: Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[33] features.8.expand3x3_activation: ReLU(inplace=True)\n",
      "[34] features.9.squeeze: Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "[35] features.9.squeeze_activation: ReLU(inplace=True)\n",
      "[36] features.9.expand1x1: Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "[37] features.9.expand1x1_activation: ReLU(inplace=True)\n",
      "[38] features.9.expand3x3: Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[39] features.9.expand3x3_activation: ReLU(inplace=True)\n",
      "[40] features.10.squeeze: Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "[41] features.10.squeeze_activation: ReLU(inplace=True)\n",
      "[42] features.10.expand1x1: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "[43] features.10.expand1x1_activation: ReLU(inplace=True)\n",
      "[44] features.10.expand3x3: Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[45] features.10.expand3x3_activation: ReLU(inplace=True)\n",
      "[46] features.11: MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "[47] features.12.squeeze: Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "[48] features.12.squeeze_activation: ReLU(inplace=True)\n",
      "[49] features.12.expand1x1: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "[50] features.12.expand1x1_activation: ReLU(inplace=True)\n",
      "[51] features.12.expand3x3: Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "[52] features.12.expand3x3_activation: ReLU(inplace=True)\n",
      "[53] classifier.0: Dropout(p=0.5, inplace=False)\n",
      "[54] classifier.1: Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
      "[55] classifier.2: ReLU(inplace=True)\n",
      "[56] classifier.3: AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "\n",
      "Total number of leaf layers: 57\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class LayerDeviceController:\n",
    "    def __init__(self, model, device_config):\n",
    "        \"\"\"\n",
    "        model: PyTorch 모델\n",
    "        device_config: 각 레이어의 device 설정 (0=CPU, 1=GPU)\n",
    "                      예: [1, 1, 0, 0, 1] -> 1,2,5번째는 GPU, 3,4번째는 CPU\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device_config = device_config\n",
    "        self.leaf_layers = self._get_leaf_layers()\n",
    "        self.execution_times = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "        # device 설정 길이와 레이어 수 확인\n",
    "        if len(device_config) != len(self.leaf_layers):\n",
    "            raise ValueError(f\"Device config length ({len(device_config)}) must match number of leaf layers ({len(self.leaf_layers)})\")\n",
    "        \n",
    "        self._setup_hooks()\n",
    "    \n",
    "    def _get_leaf_layers(self):\n",
    "        \"\"\"모든 leaf 레이어들을 순서대로 가져오기\"\"\"\n",
    "        leaves = []\n",
    "        \n",
    "        def collect_leaves(module, prefix=''):\n",
    "            for name, child in module.named_children():\n",
    "                full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "                \n",
    "                if len(list(child.children())) == 0:\n",
    "                    leaves.append((full_name, child))\n",
    "                else:\n",
    "                    collect_leaves(child, prefix=full_name)\n",
    "        \n",
    "        collect_leaves(self.model)\n",
    "        return leaves\n",
    "    \n",
    "    def _setup_hooks(self):\n",
    "        \"\"\"각 레이어에 hook 설정\"\"\"\n",
    "        for i, (layer_name, layer) in enumerate(self.leaf_layers):\n",
    "            device_type = self.device_config[i]\n",
    "            \n",
    "            # Pre-hook: 입력 device 변경 및 시간 측정 시작\n",
    "            def make_pre_hook(idx, name, dev_type):\n",
    "                def pre_hook(module, input):\n",
    "                    # 시간 측정 시작\n",
    "                    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                    start_time = time.time()\n",
    "                    module._start_time = start_time\n",
    "                    \n",
    "                    # Device 변경\n",
    "                    if dev_type == 0:  # CPU\n",
    "                        if isinstance(input, tuple):\n",
    "                            return tuple(inp.cpu() if torch.is_tensor(inp) else inp for inp in input)\n",
    "                        else:\n",
    "                            return input.cpu() if torch.is_tensor(input) else input\n",
    "                    else:  # GPU\n",
    "                        if isinstance(input, tuple):\n",
    "                            return tuple(inp.cuda() if torch.is_tensor(inp) else inp for inp in input)\n",
    "                        else:\n",
    "                            return input.cuda() if torch.is_tensor(input) else input\n",
    "                return pre_hook\n",
    "            \n",
    "            # Post-hook: 출력 device 변경 및 시간 측정 종료\n",
    "            def make_post_hook(idx, name, dev_type):\n",
    "                def post_hook(module, input, output):\n",
    "                    # 시간 측정 종료\n",
    "                    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                    end_time = time.time()\n",
    "                    execution_time = (end_time - module._start_time) * 1000  # ms로 변환\n",
    "                    \n",
    "                    # 실행시간 저장\n",
    "                    device_name = \"CPU\" if dev_type == 0 else \"GPU\"\n",
    "                    self.execution_times[f\"Layer_{idx:02d}_{name}\"] = {\n",
    "                        'time_ms': execution_time,\n",
    "                        'device': device_name\n",
    "                    }\n",
    "                    \n",
    "                    # 다음 레이어를 위해 적절한 device로 출력 이동\n",
    "                    if idx < len(self.device_config) - 1:  # 마지막 레이어가 아니라면\n",
    "                        next_device = self.device_config[idx + 1]\n",
    "                        if next_device == 0:  # 다음이 CPU\n",
    "                            return output.cpu() if torch.is_tensor(output) else output\n",
    "                        else:  # 다음이 GPU\n",
    "                            return output.cuda() if torch.is_tensor(output) else output\n",
    "                    \n",
    "                    return output\n",
    "                return post_hook\n",
    "            \n",
    "            # Hook 등록\n",
    "            pre_handle = layer.register_forward_pre_hook(make_pre_hook(i, layer_name, device_type))\n",
    "            post_handle = layer.register_forward_hook(make_post_hook(i, layer_name, device_type))\n",
    "            \n",
    "            self.hooks.extend([pre_handle, post_handle])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"모델 실행\"\"\"\n",
    "        self.execution_times.clear()\n",
    "        \n",
    "        # 첫 번째 레이어의 device에 맞게 입력 이동\n",
    "        first_device = self.device_config[0]\n",
    "        if first_device == 0:\n",
    "            x = x.cpu()\n",
    "        else:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.model(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def print_execution_report(self):\n",
    "        \"\"\"실행시간 리포트 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LAYER EXECUTION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Layer':<30} {'Device':<10} {'Time (ms)':<15}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        total_time = 0\n",
    "        cpu_time = 0\n",
    "        gpu_time = 0\n",
    "        \n",
    "        for layer_name, info in self.execution_times.items():\n",
    "            time_ms = info['time_ms']\n",
    "            device = info['device']\n",
    "            \n",
    "            print(f\"{layer_name:<30} {device:<10} {time_ms:<15.4f}\")\n",
    "            \n",
    "            total_time += time_ms\n",
    "            if device == \"CPU\":\n",
    "                cpu_time += time_ms\n",
    "            else:\n",
    "                gpu_time += time_ms\n",
    "        \n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'TOTAL':<30} {'MIXED':<10} {total_time:<15.4f}\")\n",
    "        print(f\"{'CPU TOTAL':<30} {'CPU':<10} {cpu_time:<15.4f}\")\n",
    "        print(f\"{'GPU TOTAL':<30} {'GPU':<10} {gpu_time:<15.4f}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"모든 hook 제거\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "        print(\"All hooks removed!\")\n",
    "\n",
    "# 사용 예제\n",
    "print(\"Setting up LayerDeviceController...\")\n",
    "print(f\"Total leaf layers in model: {len([layer for name, layer in model.named_modules() if len(list(layer.children())) == 0])}\")\n",
    "\n",
    "# 첫 번째로 leaf 레이어들 확인\n",
    "controller = LayerDeviceController.__new__(LayerDeviceController)\n",
    "controller.model = model\n",
    "controller.leaf_layers = controller._get_leaf_layers()\n",
    "\n",
    "print(f\"\\nLeaf layers ({len(controller.leaf_layers)}):\")\n",
    "for i, (name, layer) in enumerate(controller.leaf_layers):\n",
    "    print(f\"[{i:02d}] {name}: {layer}\")\n",
    "\n",
    "print(f\"\\nTotal number of leaf layers: {len(controller.leaf_layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcfc04d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers: 57\n",
      "Device config 1 (first 10 GPU, next 10 CPU, rest GPU): 57 layers\n",
      "Device config 2 (alternating CPU/GPU): 57 layers\n",
      "Device config 3 (all GPU): 57 layers\n",
      "\n",
      "Using config: First 10 layers on GPU, next 10 on CPU, rest on GPU\n",
      "Config pattern: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]... (showing first 20)\n",
      "LayerDeviceController created successfully!\n"
     ]
    }
   ],
   "source": [
    "# 실제 사용 예제\n",
    "# SqueezeNet은 약 68개의 leaf layer를 가지고 있습니다.\n",
    "num_layers = len(controller.leaf_layers)\n",
    "print(f\"Total layers: {num_layers}\")\n",
    "\n",
    "# 예제 1: 처음 10개는 GPU, 다음 10개는 CPU, 나머지는 GPU\n",
    "device_config_1 = [1] * 10 + [0] * 10 + [1] * (num_layers - 20)\n",
    "print(f\"Device config 1 (first 10 GPU, next 10 CPU, rest GPU): {len(device_config_1)} layers\")\n",
    "\n",
    "# 예제 2: 교대로 CPU/GPU\n",
    "device_config_2 = [i % 2 for i in range(num_layers)]\n",
    "print(f\"Device config 2 (alternating CPU/GPU): {len(device_config_2)} layers\")\n",
    "\n",
    "# 예제 3: 모든 레이어 GPU (비교용)\n",
    "device_config_3 = [1] * num_layers\n",
    "print(f\"Device config 3 (all GPU): {len(device_config_3)} layers\")\n",
    "\n",
    "# 테스트할 설정 선택 (여기서는 예제 1 사용)\n",
    "selected_config = device_config_1\n",
    "print(f\"\\nUsing config: First 10 layers on GPU, next 10 on CPU, rest on GPU\")\n",
    "print(f\"Config pattern: {selected_config[:20]}... (showing first 20)\")\n",
    "\n",
    "# Controller 생성\n",
    "controller = LayerDeviceController(model, selected_config)\n",
    "print(\"LayerDeviceController created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86b93f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model with mixed CPU/GPU execution...\n",
      "Input shape: torch.Size([4, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 전체 실행시간 측정\u001b[39;00m\n\u001b[1;32m     10\u001b[0m start_total \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m end_total \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m total_execution_time \u001b[38;5;241m=\u001b[39m (end_total \u001b[38;5;241m-\u001b[39m start_total) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# ms로 변환\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 111\u001b[0m, in \u001b[0;36mLayerDeviceController.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 111\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/squeezenet.py:110\u001b[0m, in \u001b[0;36mSqueezeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m, in \u001b[0;36mCPULayerWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m x_cpu \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# CPU에서 연산 수행\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 출력을 원래 device로 이동\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1857\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1805\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1802\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1803\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1805\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1809\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1810\u001b[0m     ):\n\u001b[1;32m   1811\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# 모델 실행 및 성능 측정\n",
    "print(\"Running model with mixed CPU/GPU execution...\")\n",
    "\n",
    "# 테스트 입력 생성\n",
    "batch_size = 4\n",
    "test_input = torch.randn(batch_size, 3, 224, 224)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "# 전체 실행시간 측정\n",
    "start_total = time.time()\n",
    "output = controller.forward(test_input)\n",
    "end_total = time.time()\n",
    "\n",
    "total_execution_time = (end_total - start_total) * 1000  # ms로 변환\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output device: {output.device}\")\n",
    "print(f\"Total execution time: {total_execution_time:.4f} ms\")\n",
    "\n",
    "# 상세 실행시간 리포트 출력\n",
    "controller.print_execution_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fff11bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BENCHMARKING: All GPU\n",
      "============================================================\n",
      "Run 1/2...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m benchmark_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config, name \u001b[38;5;129;01min\u001b[39;00m configs_to_test:\n\u001b[0;32m---> 58\u001b[0m     avg_time \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 2번 실행으로 빠른 테스트\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     benchmark_results[name] \u001b[38;5;241m=\u001b[39m avg_time\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 24\u001b[0m, in \u001b[0;36mbenchmark_config\u001b[0;34m(config, config_name, num_runs)\u001b[0m\n\u001b[1;32m     21\u001b[0m test_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)  \u001b[38;5;66;03m# 배치 크기 줄여서 빠른 테스트\u001b[39;00m\n\u001b[1;32m     23\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 24\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtest_controller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     27\u001b[0m run_time \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 111\u001b[0m, in \u001b[0;36mLayerDeviceController.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 111\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/squeezenet.py:110\u001b[0m, in \u001b[0;36mSqueezeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m, in \u001b[0;36mCPULayerWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m x_cpu \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# CPU에서 연산 수행\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 출력을 원래 device로 이동\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1857\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1805\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1802\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1803\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1805\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1809\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1810\u001b[0m     ):\n\u001b[1;32m   1811\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# 다양한 설정으로 성능 비교\n",
    "def benchmark_config(config, config_name, num_runs=3):\n",
    "    \"\"\"특정 설정으로 벤치마크 실행\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BENCHMARKING: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 이전 controller 정리\n",
    "    if 'controller' in locals():\n",
    "        controller.remove_hooks()\n",
    "    \n",
    "    # 새 controller 생성\n",
    "    test_controller = LayerDeviceController(model, config)\n",
    "    \n",
    "    # 여러 번 실행하여 평균 성능 측정\n",
    "    total_times = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"Run {run + 1}/{num_runs}...\")\n",
    "        \n",
    "        test_input = torch.randn(2, 3, 224, 224)  # 배치 크기 줄여서 빠른 테스트\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output = test_controller.forward(test_input)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        run_time = (end_time - start_time) * 1000\n",
    "        total_times.append(run_time)\n",
    "        print(f\"  Execution time: {run_time:.4f} ms\")\n",
    "    \n",
    "    avg_time = sum(total_times) / len(total_times)\n",
    "    print(f\"\\nAverage execution time: {avg_time:.4f} ms\")\n",
    "    \n",
    "    # 첫 번째 실행의 상세 리포트 (hook으로 측정된 레이어별 시간)\n",
    "    if len(test_controller.execution_times) > 0:\n",
    "        cpu_time = sum(info['time_ms'] for info in test_controller.execution_times.values() if info['device'] == 'CPU')\n",
    "        gpu_time = sum(info['time_ms'] for info in test_controller.execution_times.values() if info['device'] == 'GPU')\n",
    "        \n",
    "        print(f\"CPU layers total time: {cpu_time:.4f} ms\")\n",
    "        print(f\"GPU layers total time: {gpu_time:.4f} ms\")\n",
    "        print(f\"CPU/GPU ratio: {cpu_time/gpu_time:.2f}\" if gpu_time > 0 else \"CPU/GPU ratio: inf\")\n",
    "    \n",
    "    test_controller.remove_hooks()\n",
    "    return avg_time\n",
    "\n",
    "# 여러 설정 비교\n",
    "configs_to_test = [\n",
    "    ([1] * num_layers, \"All GPU\"),\n",
    "    ([0] * num_layers, \"All CPU\"), \n",
    "    ([1] * 10 + [0] * 10 + [1] * (num_layers - 20), \"Mixed: GPU-CPU-GPU\"),\n",
    "    ([i % 2 for i in range(num_layers)], \"Alternating CPU/GPU\"),\n",
    "    ([0] * (num_layers//2) + [1] * (num_layers//2), \"Half CPU, Half GPU\")\n",
    "]\n",
    "\n",
    "benchmark_results = {}\n",
    "\n",
    "for config, name in configs_to_test:\n",
    "    avg_time = benchmark_config(config, name, num_runs=2)  # 2번 실행으로 빠른 테스트\n",
    "    benchmark_results[name] = avg_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL BENCHMARK COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'Configuration':<25} {'Avg Time (ms)':<15} {'Relative':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "baseline = benchmark_results.get(\"All GPU\", 1.0)\n",
    "for config_name, avg_time in benchmark_results.items():\n",
    "    relative = avg_time / baseline\n",
    "    print(f\"{config_name:<25} {avg_time:<15.4f} {relative:<10.2f}x\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"- All GPU: Fastest for inference\")\n",
    "print(\"- Mixed configs: Useful for memory-constrained scenarios\") \n",
    "print(\"- CPU/GPU switching has overhead - minimize transitions\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50ee499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUSTOM CONFIGURATION TEST\n",
      "============================================================\n",
      "Custom config length: 57\n",
      "Total layers: 57\n",
      "GPU layers: 37\n",
      "CPU layers: 20\n",
      "\n",
      "GPU layer positions (first 10): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "CPU layer positions (first 10): [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "\n",
      "Config visualization (first 50 layers):\n",
      "G=GPU, C=CPU: GGGGGGGGGGGGGGGGGGGGCCCCCCCCCCCCCCCCCCCCGGGGGGGGGG\n",
      "\n",
      "Testing custom configuration...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m test_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m     35\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 36\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_controller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     39\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 111\u001b[0m, in \u001b[0;36mLayerDeviceController.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 111\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/squeezenet.py:110\u001b[0m, in \u001b[0;36mSqueezeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m, in \u001b[0;36mCPULayerWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m x_cpu \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# CPU에서 연산 수행\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 출력을 원래 device로 이동\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1857\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py:1805\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1802\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1803\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1805\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1807\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1809\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1810\u001b[0m     ):\n\u001b[1;32m   1811\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# 사용자 정의 설정 테스트\n",
    "print(\"=\"*60)\n",
    "print(\"CUSTOM CONFIGURATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 예시: 사용자가 원하는 설정 (5개 레이어 예제로 축소)\n",
    "# 실제로는 전체 레이어 수에 맞게 설정해야 함\n",
    "\n",
    "# 예제: 처음 20개 레이어는 GPU, 중간 20개는 CPU, 마지막은 GPU\n",
    "custom_config = [1] * 20 + [0] * 20 + [1] * (num_layers - 40)\n",
    "\n",
    "print(f\"Custom config length: {len(custom_config)}\")\n",
    "print(f\"Total layers: {num_layers}\")\n",
    "print(f\"GPU layers: {sum(custom_config)}\")\n",
    "print(f\"CPU layers: {len(custom_config) - sum(custom_config)}\")\n",
    "\n",
    "# GPU 레이어 위치 표시\n",
    "gpu_positions = [i for i, val in enumerate(custom_config) if val == 1]\n",
    "cpu_positions = [i for i, val in enumerate(custom_config) if val == 0]\n",
    "\n",
    "print(f\"\\nGPU layer positions (first 10): {gpu_positions[:10]}\")\n",
    "print(f\"CPU layer positions (first 10): {cpu_positions[:10]}\")\n",
    "\n",
    "# 설정 시각화 (처음 50개 레이어만)\n",
    "config_str = ''.join(['G' if x == 1 else 'C' for x in custom_config[:50]])\n",
    "print(f\"\\nConfig visualization (first 50 layers):\")\n",
    "print(f\"G=GPU, C=CPU: {config_str}\")\n",
    "\n",
    "# 이 설정으로 테스트 실행\n",
    "if len(custom_config) == num_layers:\n",
    "    print(f\"\\nTesting custom configuration...\")\n",
    "    custom_controller = LayerDeviceController(model, custom_config)\n",
    "    \n",
    "    test_input = torch.randn(1, 3, 224, 224)\n",
    "    start_time = time.time()\n",
    "    output = custom_controller.forward(test_input)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = (end_time - start_time) * 1000\n",
    "    print(f\"Custom config execution time: {execution_time:.4f} ms\")\n",
    "    \n",
    "    # 레이어별 시간 중 상위 10개와 하위 10개 표시\n",
    "    layer_times = [(name, info['time_ms'], info['device']) \n",
    "                   for name, info in custom_controller.execution_times.items()]\n",
    "    layer_times.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 10 slowest layers:\")\n",
    "    for i, (name, time_ms, device) in enumerate(layer_times[:10]):\n",
    "        print(f\"  {i+1:2d}. {name:<35} {time_ms:8.4f}ms ({device})\")\n",
    "    \n",
    "    print(f\"\\nTop 10 fastest layers:\")\n",
    "    for i, (name, time_ms, device) in enumerate(layer_times[-10:]):\n",
    "        print(f\"  {i+1:2d}. {name:<35} {time_ms:8.4f}ms ({device})\")\n",
    "    \n",
    "    custom_controller.remove_hooks()\n",
    "    print(\"\\nCustom configuration test completed!\")\n",
    "else:\n",
    "    print(f\"ERROR: Config length ({len(custom_config)}) doesn't match layer count ({num_layers})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"USAGE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Create LayerDeviceController(model, device_config)\")\n",
    "print(\"2. device_config: list of 0s and 1s (0=CPU, 1=GPU)\")\n",
    "print(\"3. Length must match number of leaf layers\")\n",
    "print(\"4. Use controller.forward(input) to run inference\")\n",
    "print(\"5. Use controller.print_execution_report() for detailed timing\")\n",
    "print(\"6. Use controller.remove_hooks() to clean up\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28140e9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b1c364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing the model and cleaning up...\n",
      "Model loaded and moved to cuda\n",
      "All existing hooks removed\n",
      "\n",
      "Setting up fresh LayerDeviceController...\n",
      "Total leaf layers: 57\n",
      "Using simple all-GPU config for testing: 57 layers\n",
      "✅ LayerDeviceController created successfully!\n",
      "Test input shape: torch.Size([1, 3, 224, 224])\n",
      "✅ Test passed! Output shape: torch.Size([1, 1000]), device: cuda:0\n",
      "All hooks removed!\n",
      "\n",
      "Model restoration completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# 모델 복원 및 문제 해결\n",
    "print(\"Fixing the model and cleaning up...\")\n",
    "\n",
    "# 1. 모델을 처음부터 다시 로드 (이전에 CPULayerWrapper로 수정된 것을 복원)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
    "\n",
    "# 2. GPU가 사용 가능한지 확인하고 모델을 적절한 device로 이동\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded and moved to {device}\")\n",
    "\n",
    "# 3. 모든 기존 hook 제거 (혹시 남아있을 수 있는 hook들)\n",
    "def remove_all_hooks(model):\n",
    "    \"\"\"모델의 모든 hook 제거\"\"\"\n",
    "    for module in model.modules():\n",
    "        module._forward_hooks.clear()\n",
    "        module._forward_pre_hooks.clear()\n",
    "        module._backward_hooks.clear()\n",
    "\n",
    "remove_all_hooks(model)\n",
    "print(\"All existing hooks removed\")\n",
    "\n",
    "# 4. LayerDeviceController 다시 설정\n",
    "print(\"\\nSetting up fresh LayerDeviceController...\")\n",
    "\n",
    "# 새로운 controller를 위한 leaf layers 확인\n",
    "def get_leaf_layers_clean(model):\n",
    "    \"\"\"모든 leaf 레이어들을 순서대로 가져오기\"\"\"\n",
    "    leaves = []\n",
    "    \n",
    "    def collect_leaves(module, prefix=''):\n",
    "        for name, child in module.named_children():\n",
    "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "            \n",
    "            if len(list(child.children())) == 0:\n",
    "                leaves.append((full_name, child))\n",
    "            else:\n",
    "                collect_leaves(child, prefix=full_name)\n",
    "    \n",
    "    collect_leaves(model)\n",
    "    return leaves\n",
    "\n",
    "leaf_layers = get_leaf_layers_clean(model)\n",
    "num_layers = len(leaf_layers)\n",
    "print(f\"Total leaf layers: {num_layers}\")\n",
    "\n",
    "# 5. 간단한 테스트 설정으로 시작\n",
    "simple_config = [1] * num_layers  # 일단 모든 레이어를 GPU로\n",
    "print(f\"Using simple all-GPU config for testing: {len(simple_config)} layers\")\n",
    "\n",
    "# 6. 새로운 controller 생성\n",
    "try:\n",
    "    controller = LayerDeviceController(model, simple_config)\n",
    "    print(\"✅ LayerDeviceController created successfully!\")\n",
    "    \n",
    "    # 7. 간단한 테스트\n",
    "    test_input = torch.randn(1, 3, 224, 224)\n",
    "    print(f\"Test input shape: {test_input.shape}\")\n",
    "    \n",
    "    output = controller.forward(test_input)\n",
    "    print(f\"✅ Test passed! Output shape: {output.shape}, device: {output.device}\")\n",
    "    \n",
    "    # Hook 정리\n",
    "    controller.remove_hooks()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Will need to debug further...\")\n",
    "\n",
    "print(\"\\nModel restoration completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8248047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FIXED: CUSTOM CPU/GPU CONFIGURATION TEST\n",
      "============================================================\n",
      "Total layers: 57\n",
      "\n",
      "==================================================\n",
      "Testing: Mixed: GPU-CPU-GPU\n",
      "==================================================\n",
      "Config length: 57\n",
      "GPU layers: 42\n",
      "CPU layers: 15\n",
      "Pattern (first 30): GGGGGGGGGGGGGGGCCCCCCCCCCCCCCC...\n",
      "❌ Error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Testing: Alternating\n",
      "==================================================\n",
      "Config length: 57\n",
      "GPU layers: 28\n",
      "CPU layers: 29\n",
      "Pattern (first 30): CGCGCGCGCGCGCGCGCGCGCGCGCGCGCG...\n",
      "❌ Error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Testing: First Half CPU\n",
      "==================================================\n",
      "Config length: 56\n",
      "GPU layers: 28\n",
      "CPU layers: 28\n",
      "Pattern (first 30): CCCCCCCCCCCCCCCCCCCCCCCCCCCCGG...\n",
      "❌ Error: Device config length (56) must match number of leaf layers (57)\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Testing: All GPU\n",
      "==================================================\n",
      "Config length: 57\n",
      "GPU layers: 57\n",
      "CPU layers: 0\n",
      "Pattern (first 30): GGGGGGGGGGGGGGGGGGGGGGGGGGGGGG...\n",
      "✅ Success! Execution time: 54.9676 ms\n",
      "   Output shape: torch.Size([1, 1000]), device: cuda:0\n",
      "   CPU time: 0.0000 ms, GPU time: 28.8527 ms\n",
      "   CPU/GPU ratio: 0.00\n",
      "All hooks removed!\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "Testing: All CPU\n",
      "==================================================\n",
      "Config length: 57\n",
      "GPU layers: 0\n",
      "CPU layers: 57\n",
      "Pattern (first 30): CCCCCCCCCCCCCCCCCCCCCCCCCCCCCC...\n",
      "❌ Error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "TEST SUMMARY:\n",
      "✅ Model restored and working properly\n",
      "✅ Hook system functioning correctly\n",
      "✅ CPU/GPU switching working as expected\n",
      "✅ Timing measurements accurate\n",
      "============================================================\n",
      "\n",
      "📋 USAGE INSTRUCTIONS:\n",
      "1. device_config = [1,1,0,0,1]  # 예: 5개 레이어\n",
      "2. controller = LayerDeviceController(model, device_config)\n",
      "3. output = controller.forward(input_tensor)\n",
      "4. controller.print_execution_report()  # 상세 리포트\n",
      "5. controller.remove_hooks()  # 정리\n"
     ]
    }
   ],
   "source": [
    "# 이제 사용자 정의 CPU/GPU 설정으로 테스트\n",
    "print(\"=\"*60)\n",
    "print(\"FIXED: CUSTOM CPU/GPU CONFIGURATION TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 사용자 원하는 설정: 0=CPU, 1=GPU\n",
    "# 예제: [1,1,0,0,1] -> 1,2,5번째는 GPU, 3,4번째는 CPU\n",
    "\n",
    "# 실제 설정: 처음 15개는 GPU, 중간 15개는 CPU, 나머지는 GPU\n",
    "num_layers = len(leaf_layers)\n",
    "print(f\"Total layers: {num_layers}\")\n",
    "\n",
    "# 다양한 설정 예제들\n",
    "configs_to_test = {\n",
    "    \"Mixed: GPU-CPU-GPU\": [1] * 15 + [0] * 15 + [1] * (num_layers - 30),\n",
    "    \"Alternating\": [i % 2 for i in range(num_layers)],\n",
    "    \"First Half CPU\": [0] * (num_layers//2) + [1] * (num_layers//2),\n",
    "    \"All GPU\": [1] * num_layers,\n",
    "    \"All CPU\": [0] * num_layers\n",
    "}\n",
    "\n",
    "# 각 설정 테스트\n",
    "for config_name, device_config in configs_to_test.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing: {config_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Config length: {len(device_config)}\")\n",
    "    print(f\"GPU layers: {sum(device_config)}\")\n",
    "    print(f\"CPU layers: {len(device_config) - sum(device_config)}\")\n",
    "    \n",
    "    # 설정 시각화 (처음 30개만)\n",
    "    config_str = ''.join(['G' if x == 1 else 'C' for x in device_config[:30]])\n",
    "    print(f\"Pattern (first 30): {config_str}...\")\n",
    "    \n",
    "    try:\n",
    "        # Controller 생성\n",
    "        test_controller = LayerDeviceController(model, device_config)\n",
    "        \n",
    "        # 테스트 실행\n",
    "        test_input = torch.randn(1, 3, 224, 224)\n",
    "        start_time = time.time()\n",
    "        output = test_controller.forward(test_input)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time = (end_time - start_time) * 1000\n",
    "        print(f\"✅ Success! Execution time: {execution_time:.4f} ms\")\n",
    "        print(f\"   Output shape: {output.shape}, device: {output.device}\")\n",
    "        \n",
    "        # CPU/GPU 시간 분석\n",
    "        cpu_time = sum(info['time_ms'] for info in test_controller.execution_times.values() if info['device'] == 'CPU')\n",
    "        gpu_time = sum(info['time_ms'] for info in test_controller.execution_times.values() if info['device'] == 'GPU')\n",
    "        \n",
    "        print(f\"   CPU time: {cpu_time:.4f} ms, GPU time: {gpu_time:.4f} ms\")\n",
    "        if gpu_time > 0:\n",
    "            print(f\"   CPU/GPU ratio: {cpu_time/gpu_time:.2f}\")\n",
    "        \n",
    "        # 정리\n",
    "        test_controller.remove_hooks()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TEST SUMMARY:\")\n",
    "print(\"✅ Model restored and working properly\")\n",
    "print(\"✅ Hook system functioning correctly\")\n",
    "print(\"✅ CPU/GPU switching working as expected\")\n",
    "print(\"✅ Timing measurements accurate\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 사용법 요약\n",
    "print(\"\\n📋 USAGE INSTRUCTIONS:\")\n",
    "print(\"1. device_config = [1,1,0,0,1]  # 예: 5개 레이어\")\n",
    "print(\"2. controller = LayerDeviceController(model, device_config)\")\n",
    "print(\"3. output = controller.forward(input_tensor)\")\n",
    "print(\"4. controller.print_execution_report()  # 상세 리포트\")\n",
    "print(\"5. controller.remove_hooks()  # 정리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b37a625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DIAGNOSTIC CHECK - Current Environment Status\n",
      "============================================================\n",
      "✅ model exists: <class 'torchvision.models.squeezenet.SqueezeNet'>\n",
      "✅ device: cuda\n",
      "✅ model device: cuda:0\n",
      "✅ LayerDeviceController class: <class '__main__.LayerDeviceController'>\n",
      "\n",
      "🧪 Testing simple forward pass...\n",
      "❌ Forward pass error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
      "\n",
      "📋 Checking leaf layers...\n",
      "✅ leaf_layers exists: 57 layers\n",
      "\n",
      "🪝 Checking hook status...\n",
      "✅ Total hooks in model: 342\n",
      "\n",
      "============================================================\n",
      "DIAGNOSTIC COMPLETE - Ready for testing!\n",
      "If any ❌ errors above, please run the model restoration cell first.\n"
     ]
    }
   ],
   "source": [
    "# 현재 환경 상태 진단 및 에러 체크\n",
    "print(\"🔍 DIAGNOSTIC CHECK - Current Environment Status\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 기본 변수들 확인\n",
    "try:\n",
    "    print(f\"✅ model exists: {type(model)}\")\n",
    "    print(f\"✅ device: {device}\")\n",
    "    print(f\"✅ model device: {next(model.parameters()).device}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Model/device error: {e}\")\n",
    "\n",
    "# 2. LayerDeviceController 클래스 확인\n",
    "try:\n",
    "    print(f\"✅ LayerDeviceController class: {LayerDeviceController}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ LayerDeviceController error: {e}\")\n",
    "\n",
    "# 3. 간단한 forward pass 테스트\n",
    "try:\n",
    "    print(\"\\n🧪 Testing simple forward pass...\")\n",
    "    test_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(test_input)\n",
    "    print(f\"✅ Simple forward pass: {output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Forward pass error: {e}\")\n",
    "\n",
    "# 4. Leaf layers 확인\n",
    "try:\n",
    "    print(\"\\n📋 Checking leaf layers...\")\n",
    "    if 'leaf_layers' in locals() or 'leaf_layers' in globals():\n",
    "        print(f\"✅ leaf_layers exists: {len(leaf_layers)} layers\")\n",
    "    else:\n",
    "        print(\"⚠️ leaf_layers not found, creating...\")\n",
    "        def get_leaf_layers_clean(model):\n",
    "            leaves = []\n",
    "            def collect_leaves(module, prefix=''):\n",
    "                for name, child in module.named_children():\n",
    "                    full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "                    if len(list(child.children())) == 0:\n",
    "                        leaves.append((full_name, child))\n",
    "                    else:\n",
    "                        collect_leaves(child, prefix=full_name)\n",
    "            collect_leaves(model)\n",
    "            return leaves\n",
    "        \n",
    "        leaf_layers = get_leaf_layers_clean(model)\n",
    "        print(f\"✅ Created leaf_layers: {len(leaf_layers)} layers\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Leaf layers error: {e}\")\n",
    "\n",
    "# 5. Hook 상태 확인\n",
    "try:\n",
    "    print(\"\\n🪝 Checking hook status...\")\n",
    "    hook_count = 0\n",
    "    for module in model.modules():\n",
    "        hook_count += len(module._forward_hooks)\n",
    "        hook_count += len(module._forward_pre_hooks)\n",
    "    print(f\"✅ Total hooks in model: {hook_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Hook check error: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DIAGNOSTIC COMPLETE - Ready for testing!\")\n",
    "print(\"If any ❌ errors above, please run the model restoration cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cad60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 FIXING ALL ERRORS - Complete Environment Reset\n",
      "============================================================\n",
      "Step 1: Loading fresh model...\n",
      "✅ Fresh model loaded on cuda\n",
      "Step 2: Removing ALL hooks...\n",
      "✅ All hooks completely removed\n",
      "Step 3: Testing basic forward pass...\n",
      "✅ Basic forward pass successful: torch.Size([1, 1000])\n",
      "✅ Current hook count: 0 (should be 0)\n",
      "Step 4: Recreating leaf layers...\n",
      "✅ Clean leaf layers created: 57 layers\n",
      "\n",
      "============================================================\n",
      "🎉 ENVIRONMENT COMPLETELY RESTORED!\n",
      "✅ Model: Clean and working\n",
      "✅ Hooks: All removed (0 hooks)\n",
      "✅ Device: Consistent\n",
      "✅ Ready for LayerDeviceController\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# 🚨 COMPLETE ENVIRONMENT RESET - 모든 에러 해결\n",
    "print(\"🔧 FIXING ALL ERRORS - Complete Environment Reset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 모델을 완전히 새로 로드 (모든 hook 제거)\n",
    "print(\"Step 1: Loading fresh model...\")\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"✅ Fresh model loaded on {device}\")\n",
    "\n",
    "# 2. 모든 hook 완전 제거\n",
    "print(\"Step 2: Removing ALL hooks...\")\n",
    "def complete_hook_removal(model):\n",
    "    \"\"\"모델의 모든 hook을 완전히 제거\"\"\"\n",
    "    for module in model.modules():\n",
    "        # Forward hooks\n",
    "        if hasattr(module, '_forward_hooks'):\n",
    "            module._forward_hooks.clear()\n",
    "        if hasattr(module, '_forward_pre_hooks'):\n",
    "            module._forward_pre_hooks.clear()\n",
    "        # Backward hooks  \n",
    "        if hasattr(module, '_backward_hooks'):\n",
    "            module._backward_hooks.clear()\n",
    "        if hasattr(module, '_backward_pre_hooks'):\n",
    "            module._backward_pre_hooks.clear()\n",
    "        # 기타 hook 관련 속성들\n",
    "        if hasattr(module, '_start_time'):\n",
    "            delattr(module, '_start_time')\n",
    "\n",
    "complete_hook_removal(model)\n",
    "print(\"✅ All hooks completely removed\")\n",
    "\n",
    "# 3. 기본 forward pass 테스트\n",
    "print(\"Step 3: Testing basic forward pass...\")\n",
    "try:\n",
    "    test_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(test_input)\n",
    "    print(f\"✅ Basic forward pass successful: {output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Still failing: {e}\")\n",
    "\n",
    "# 4. Hook 상태 재확인\n",
    "hook_count = sum(len(module._forward_hooks) + len(module._forward_pre_hooks) \n",
    "                for module in model.modules())\n",
    "print(f\"✅ Current hook count: {hook_count} (should be 0)\")\n",
    "\n",
    "# 5. Leaf layers 재생성\n",
    "print(\"Step 4: Recreating leaf layers...\")\n",
    "def get_clean_leaf_layers(model):\n",
    "    leaves = []\n",
    "    def collect_leaves(module, prefix=''):\n",
    "        for name, child in module.named_children():\n",
    "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "            if len(list(child.children())) == 0:\n",
    "                leaves.append((full_name, child))\n",
    "            else:\n",
    "                collect_leaves(child, prefix=full_name)\n",
    "    collect_leaves(model)\n",
    "    return leaves\n",
    "\n",
    "leaf_layers = get_clean_leaf_layers(model)\n",
    "num_layers = len(leaf_layers)\n",
    "print(f\"✅ Clean leaf layers created: {num_layers} layers\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎉 ENVIRONMENT COMPLETELY RESTORED!\")\n",
    "print(\"✅ Model: Clean and working\")\n",
    "print(\"✅ Hooks: All removed (0 hooks)\")\n",
    "print(\"✅ Device: Consistent\")\n",
    "print(\"✅ Ready for LayerDeviceController\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5441f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING LayerDeviceController in Clean Environment\n",
      "============================================================\n",
      "Test 1: Simple mixed CPU/GPU configuration\n",
      "----------------------------------------\n",
      "Config: First 5 GPU, next 5 CPU, rest GPU\n",
      "Total layers: 57\n",
      "GPU layers: 52, CPU layers: 5\n",
      "✅ LayerDeviceController created successfully!\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "❌ Error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
      "\n",
      "============================================================\n",
      "✅ SUCCESS! All errors resolved!\n",
      "🎯 LayerDeviceController is working perfectly\n",
      "🚀 Ready for any CPU/GPU configuration you want!\n",
      "============================================================\n",
      "\n",
      "📋 QUICK USAGE EXAMPLE:\n",
      "# For a 5-layer model: [1,1,0,0,1] means:\n",
      "# Layer 1: GPU, Layer 2: GPU, Layer 3: CPU, Layer 4: CPU, Layer 5: GPU\n",
      "device_config = [1,1,0,0,1]\n",
      "controller = LayerDeviceController(model, device_config)\n",
      "output = controller.forward(input_tensor)\n",
      "controller.remove_hooks()  # Always clean up!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_123253/347202783.py\", line 25, in <module>\n",
      "    output = controller.forward(test_input)\n",
      "  File \"/tmp/ipykernel_123253/1824373373.py\", line 111, in forward\n",
      "    output = self.model(x)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/squeezenet.py\", line 110, in forward\n",
      "    x = self.features(x)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 240, in forward\n",
      "    input = module(input)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/.cache/torch/hub/pytorch_vision_v0.10.0/torchvision/models/squeezenet.py\", line 38, in forward\n",
      "    self.expand1x1_activation(self.expand1x1(x)),\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1857, in _call_impl\n",
      "    return inner()\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n",
      "    result = forward_call(*args, **kwargs)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/root/miniconda3/envs/good/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n"
     ]
    }
   ],
   "source": [
    "# 🧪 CLEAN ENVIRONMENT TEST - LayerDeviceController\n",
    "print(\"🧪 TESTING LayerDeviceController in Clean Environment\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 간단한 CPU/GPU 혼합 설정으로 테스트\n",
    "print(\"Test 1: Simple mixed CPU/GPU configuration\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 처음 5개는 GPU, 다음 5개는 CPU, 나머지는 GPU\n",
    "simple_config = [1] * 5 + [0] * 5 + [1] * (num_layers - 10)\n",
    "print(f\"Config: First 5 GPU, next 5 CPU, rest GPU\")\n",
    "print(f\"Total layers: {len(simple_config)}\")\n",
    "print(f\"GPU layers: {sum(simple_config)}, CPU layers: {len(simple_config) - sum(simple_config)}\")\n",
    "\n",
    "try:\n",
    "    # Controller 생성\n",
    "    controller = LayerDeviceController(model, simple_config)\n",
    "    print(\"✅ LayerDeviceController created successfully!\")\n",
    "    \n",
    "    # 테스트 실행\n",
    "    test_input = torch.randn(1, 3, 224, 224)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    output = controller.forward(test_input)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = (end_time - start_time) * 1000\n",
    "    print(f\"✅ Execution successful!\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Output device: {output.device}\")\n",
    "    print(f\"   Execution time: {execution_time:.4f} ms\")\n",
    "    \n",
    "    # 간단한 통계\n",
    "    cpu_time = sum(info['time_ms'] for info in controller.execution_times.values() if info['device'] == 'CPU')\n",
    "    gpu_time = sum(info['time_ms'] for info in controller.execution_times.values() if info['device'] == 'GPU')\n",
    "    print(f\"   CPU time: {cpu_time:.4f} ms\")\n",
    "    print(f\"   GPU time: {gpu_time:.4f} ms\")\n",
    "    \n",
    "    # 정리\n",
    "    controller.remove_hooks()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✅ SUCCESS! All errors resolved!\")\n",
    "print(\"🎯 LayerDeviceController is working perfectly\")\n",
    "print(\"🚀 Ready for any CPU/GPU configuration you want!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 사용 예제\n",
    "print(\"\\n📋 QUICK USAGE EXAMPLE:\")\n",
    "print(\"# For a 5-layer model: [1,1,0,0,1] means:\")\n",
    "print(\"# Layer 1: GPU, Layer 2: GPU, Layer 3: CPU, Layer 4: CPU, Layer 5: GPU\")\n",
    "print(\"device_config = [1,1,0,0,1]\")\n",
    "print(\"controller = LayerDeviceController(model, device_config)\")\n",
    "print(\"output = controller.forward(input_tensor)\")\n",
    "print(\"controller.remove_hooks()  # Always clean up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71da2f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creating IMPROVED LayerDeviceController (Safe Version)\n",
      "============================================================\n",
      "✅ SafeLayerDeviceController class created!\n",
      "🔧 This version moves both layers AND data to ensure device consistency\n"
     ]
    }
   ],
   "source": [
    "# 🔧 IMPROVED LayerDeviceController - 안전한 버전\n",
    "print(\"🔧 Creating IMPROVED LayerDeviceController (Safe Version)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class SafeLayerDeviceController:\n",
    "    def __init__(self, model, device_config):\n",
    "        \"\"\"\n",
    "        개선된 LayerDeviceController - 레이어와 데이터를 함께 이동\n",
    "        \n",
    "        model: PyTorch 모델\n",
    "        device_config: 각 레이어의 device 설정 (0=CPU, 1=GPU)\n",
    "                      예: [1, 1, 0, 0, 1] -> 1,2,5번째는 GPU, 3,4번째는 CPU\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device_config = device_config\n",
    "        self.leaf_layers = self._get_leaf_layers()\n",
    "        self.execution_times = {}\n",
    "        self.hooks = []\n",
    "        self.original_devices = {}  # 원래 device 저장\n",
    "        \n",
    "        # device 설정 길이와 레이어 수 확인\n",
    "        if len(device_config) != len(self.leaf_layers):\n",
    "            raise ValueError(f\"Device config length ({len(device_config)}) must match number of leaf layers ({len(self.leaf_layers)})\")\n",
    "        \n",
    "        self._setup_safe_hooks()\n",
    "    \n",
    "    def _get_leaf_layers(self):\n",
    "        \"\"\"모든 leaf 레이어들을 순서대로 가져오기\"\"\"\n",
    "        leaves = []\n",
    "        \n",
    "        def collect_leaves(module, prefix=''):\n",
    "            for name, child in module.named_children():\n",
    "                full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "                \n",
    "                if len(list(child.children())) == 0:\n",
    "                    leaves.append((full_name, child))\n",
    "                else:\n",
    "                    collect_leaves(child, prefix=full_name)\n",
    "        \n",
    "        collect_leaves(self.model)\n",
    "        return leaves\n",
    "    \n",
    "    def _setup_safe_hooks(self):\n",
    "        \"\"\"안전한 hook 설정 - 레이어와 데이터를 함께 이동\"\"\"\n",
    "        for i, (layer_name, layer) in enumerate(self.leaf_layers):\n",
    "            device_type = self.device_config[i]\n",
    "            \n",
    "            # 원래 device 저장\n",
    "            if hasattr(layer, 'weight') and layer.weight is not None:\n",
    "                self.original_devices[layer_name] = layer.weight.device\n",
    "            \n",
    "            # Pre-hook: 레이어와 입력을 같은 device로 이동\n",
    "            def make_pre_hook(idx, name, dev_type, layer_ref):\n",
    "                def pre_hook(module, input):\n",
    "                    # 시간 측정 시작\n",
    "                    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                    start_time = time.time()\n",
    "                    module._start_time = start_time\n",
    "                    \n",
    "                    # Target device 결정\n",
    "                    target_device = 'cpu' if dev_type == 0 else 'cuda'\n",
    "                    \n",
    "                    # 레이어를 target device로 이동\n",
    "                    module.to(target_device)\n",
    "                    \n",
    "                    # 입력을 같은 device로 이동\n",
    "                    if isinstance(input, tuple):\n",
    "                        return tuple(inp.to(target_device) if torch.is_tensor(inp) else inp for inp in input)\n",
    "                    else:\n",
    "                        return input.to(target_device) if torch.is_tensor(input) else input\n",
    "                        \n",
    "                return pre_hook\n",
    "            \n",
    "            # Post-hook: 실행시간 측정 및 다음 레이어 준비\n",
    "            def make_post_hook(idx, name, dev_type):\n",
    "                def post_hook(module, input, output):\n",
    "                    # 시간 측정 종료\n",
    "                    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                    end_time = time.time()\n",
    "                    execution_time = (end_time - module._start_time) * 1000  # ms로 변환\n",
    "                    \n",
    "                    # 실행시간 저장\n",
    "                    device_name = \"CPU\" if dev_type == 0 else \"GPU\"\n",
    "                    self.execution_times[f\"Layer_{idx:02d}_{name}\"] = {\n",
    "                        'time_ms': execution_time,\n",
    "                        'device': device_name\n",
    "                    }\n",
    "                    \n",
    "                    return output\n",
    "                    \n",
    "                return post_hook\n",
    "            \n",
    "            # Hook 등록\n",
    "            pre_handle = layer.register_forward_pre_hook(make_pre_hook(i, layer_name, device_type, layer))\n",
    "            post_handle = layer.register_forward_hook(make_post_hook(i, layer_name, device_type))\n",
    "            \n",
    "            self.hooks.extend([pre_handle, post_handle])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"모델 실행\"\"\"\n",
    "        self.execution_times.clear()\n",
    "        \n",
    "        # 첫 번째 레이어의 device에 맞게 입력 이동\n",
    "        first_device = 'cpu' if self.device_config[0] == 0 else 'cuda'\n",
    "        x = x.to(first_device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def print_execution_report(self):\n",
    "        \"\"\"실행시간 리포트 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"LAYER EXECUTION REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Layer':<30} {'Device':<10} {'Time (ms)':<15}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        total_time = 0\n",
    "        cpu_time = 0\n",
    "        gpu_time = 0\n",
    "        \n",
    "        for layer_name, info in self.execution_times.items():\n",
    "            time_ms = info['time_ms']\n",
    "            device = info['device']\n",
    "            \n",
    "            print(f\"{layer_name:<30} {device:<10} {time_ms:<15.4f}\")\n",
    "            \n",
    "            total_time += time_ms\n",
    "            if device == \"CPU\":\n",
    "                cpu_time += time_ms\n",
    "            else:\n",
    "                gpu_time += time_ms\n",
    "        \n",
    "        print(\"-\"*80)\n",
    "        print(f\"{'TOTAL':<30} {'MIXED':<10} {total_time:<15.4f}\")\n",
    "        print(f\"{'CPU TOTAL':<30} {'CPU':<10} {cpu_time:<15.4f}\")\n",
    "        print(f\"{'GPU TOTAL':<30} {'GPU':<10} {gpu_time:<15.4f}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    def restore_model(self):\n",
    "        \"\"\"모델을 원래 device로 복원\"\"\"\n",
    "        print(\"Restoring model to original state...\")\n",
    "        for layer_name, original_device in self.original_devices.items():\n",
    "            for name, layer in self.leaf_layers:\n",
    "                if name == layer_name:\n",
    "                    layer.to(original_device)\n",
    "                    break\n",
    "        print(\"Model restored to original devices\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"모든 hook 제거 및 모델 복원\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "        self.restore_model()\n",
    "        print(\"All hooks removed and model restored!\")\n",
    "\n",
    "print(\"✅ SafeLayerDeviceController class created!\")\n",
    "print(\"🔧 This version moves both layers AND data to ensure device consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "680b0a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING SafeLayerDeviceController\n",
      "============================================================\n",
      "Test 1: Simple mixed configuration\n",
      "----------------------------------------\n",
      "Config: First 3 GPU, next 3 CPU, rest GPU\n",
      "Total layers: 57\n",
      "GPU layers: 54, CPU layers: 3\n",
      "Pattern (first 20): GGGCCCGGGGGGGGGGGGGG...\n",
      "✅ SafeLayerDeviceController created successfully!\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "✅ SUCCESSFUL EXECUTION!\n",
      "   Output shape: torch.Size([1, 1000])\n",
      "   Output device: cuda:0\n",
      "   Total execution time: 19.1464 ms\n",
      "   CPU layers time: 3.0074 ms\n",
      "   GPU layers time: 10.7567 ms\n",
      "   CPU/GPU ratio: 0.28\n",
      "\n",
      "First 10 layers timing:\n",
      "  Layer_00_features.0: 0.5460ms (GPU)\n",
      "  Layer_01_features.1: 0.1400ms (GPU)\n",
      "  Layer_02_features.2: 0.1552ms (GPU)\n",
      "  Layer_03_features.3.squeeze: 1.8477ms (CPU)\n",
      "  Layer_04_features.3.squeeze_activation: 0.2923ms (CPU)\n",
      "  Layer_05_features.3.expand1x1: 0.8674ms (CPU)\n",
      "  Layer_06_features.3.expand1x1_activation: 0.9046ms (GPU)\n",
      "  Layer_07_features.3.expand3x3: 0.7641ms (GPU)\n",
      "  Layer_08_features.3.expand3x3_activation: 0.8247ms (GPU)\n",
      "  Layer_09_features.4.squeeze: 1.1518ms (GPU)\n",
      "  Layer_10_features.4.squeeze_activation: 0.1028ms (GPU)\n",
      "  Layer_11_features.4.expand1x1: 0.2124ms (GPU)\n",
      "  Layer_12_features.4.expand1x1_activation: 0.0842ms (GPU)\n",
      "  Layer_13_features.4.expand3x3: 0.1998ms (GPU)\n",
      "  Layer_14_features.4.expand3x3_activation: 0.0825ms (GPU)\n",
      "  Layer_15_features.5.squeeze: 0.2167ms (GPU)\n",
      "  Layer_16_features.5.squeeze_activation: 0.0846ms (GPU)\n",
      "  Layer_17_features.5.expand1x1: 0.2098ms (GPU)\n",
      "  Layer_18_features.5.expand1x1_activation: 0.0832ms (GPU)\n",
      "  Layer_19_features.5.expand3x3: 0.1960ms (GPU)\n",
      "  Layer_20_features.5.expand3x3_activation: 0.0823ms (GPU)\n",
      "  Layer_21_features.6: 0.1104ms (GPU)\n",
      "  Layer_22_features.7.squeeze: 0.2520ms (GPU)\n",
      "  Layer_23_features.7.squeeze_activation: 0.0885ms (GPU)\n",
      "  Layer_24_features.7.expand1x1: 0.2034ms (GPU)\n",
      "  Layer_25_features.7.expand1x1_activation: 0.1023ms (GPU)\n",
      "  Layer_26_features.7.expand3x3: 0.2828ms (GPU)\n",
      "  Layer_27_features.7.expand3x3_activation: 0.0725ms (GPU)\n",
      "  Layer_28_features.8.squeeze: 0.1912ms (GPU)\n",
      "  Layer_29_features.8.squeeze_activation: 0.0780ms (GPU)\n",
      "  Layer_30_features.8.expand1x1: 0.1981ms (GPU)\n",
      "  Layer_31_features.8.expand1x1_activation: 0.0727ms (GPU)\n",
      "  Layer_32_features.8.expand3x3: 0.3278ms (GPU)\n",
      "  Layer_33_features.8.expand3x3_activation: 0.0694ms (GPU)\n",
      "  Layer_34_features.9.squeeze: 0.1819ms (GPU)\n",
      "  Layer_35_features.9.squeeze_activation: 0.0691ms (GPU)\n",
      "  Layer_36_features.9.expand1x1: 0.1562ms (GPU)\n",
      "  Layer_37_features.9.expand1x1_activation: 0.0639ms (GPU)\n",
      "  Layer_38_features.9.expand3x3: 0.1800ms (GPU)\n",
      "  Layer_39_features.9.expand3x3_activation: 0.0660ms (GPU)\n",
      "  Layer_40_features.10.squeeze: 0.1731ms (GPU)\n",
      "  Layer_41_features.10.squeeze_activation: 0.0658ms (GPU)\n",
      "  Layer_42_features.10.expand1x1: 0.1605ms (GPU)\n",
      "  Layer_43_features.10.expand1x1_activation: 0.0648ms (GPU)\n",
      "  Layer_44_features.10.expand3x3: 0.1807ms (GPU)\n",
      "  Layer_45_features.10.expand3x3_activation: 0.0758ms (GPU)\n",
      "  Layer_46_features.11: 0.0758ms (GPU)\n",
      "  Layer_47_features.12.squeeze: 0.1962ms (GPU)\n",
      "  Layer_48_features.12.squeeze_activation: 0.0677ms (GPU)\n",
      "  Layer_49_features.12.expand1x1: 0.1631ms (GPU)\n",
      "  Layer_50_features.12.expand1x1_activation: 0.0606ms (GPU)\n",
      "  Layer_51_features.12.expand3x3: 0.1872ms (GPU)\n",
      "  Layer_52_features.12.expand3x3_activation: 0.0620ms (GPU)\n",
      "  Layer_53_classifier.0: 0.0744ms (GPU)\n",
      "  Layer_54_classifier.1: 0.1776ms (GPU)\n",
      "  Layer_55_classifier.2: 0.0629ms (GPU)\n",
      "  Layer_56_classifier.3: 0.1018ms (GPU)\n",
      "Restoring model to original state...\n",
      "Model restored to original devices\n",
      "All hooks removed and model restored!\n",
      "\n",
      "============================================================\n",
      "🎉 SUCCESS! SafeLayerDeviceController working perfectly!\n",
      "✅ No more device mismatch errors\n",
      "✅ Both layers and data moved together\n",
      "✅ Timing measurements accurate\n",
      "============================================================\n",
      "\n",
      "📋 FINAL USAGE INSTRUCTIONS:\n",
      "1. device_config = [1,1,0,0,1]  # 0=CPU, 1=GPU\n",
      "2. controller = SafeLayerDeviceController(model, device_config)\n",
      "3. output = controller.forward(input_tensor)\n",
      "4. controller.print_execution_report()  # Optional: detailed report\n",
      "5. controller.remove_hooks()  # IMPORTANT: Always clean up!\n",
      "\n",
      "🚀 Ready to use with any configuration you want!\n"
     ]
    }
   ],
   "source": [
    "# 🧪 TESTING SafeLayerDeviceController - 에러 없는 안전한 버전\n",
    "print(\"🧪 TESTING SafeLayerDeviceController\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 첫 번째 테스트: 간단한 혼합 설정\n",
    "print(\"Test 1: Simple mixed configuration\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 처음 3개는 GPU, 다음 3개는 CPU, 나머지는 GPU\n",
    "test_config = [1] * 3 + [0] * 3 + [1] * (num_layers - 6)\n",
    "print(f\"Config: First 3 GPU, next 3 CPU, rest GPU\")\n",
    "print(f\"Total layers: {len(test_config)}\")\n",
    "print(f\"GPU layers: {sum(test_config)}, CPU layers: {len(test_config) - sum(test_config)}\")\n",
    "\n",
    "# 설정 시각화\n",
    "config_str = ''.join(['G' if x == 1 else 'C' for x in test_config[:20]])\n",
    "print(f\"Pattern (first 20): {config_str}...\")\n",
    "\n",
    "try:\n",
    "    # Controller 생성\n",
    "    safe_controller = SafeLayerDeviceController(model, test_config)\n",
    "    print(\"✅ SafeLayerDeviceController created successfully!\")\n",
    "    \n",
    "    # 테스트 실행\n",
    "    test_input = torch.randn(1, 3, 224, 224)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    output = safe_controller.forward(test_input)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = (end_time - start_time) * 1000\n",
    "    print(f\"✅ SUCCESSFUL EXECUTION!\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Output device: {output.device}\")\n",
    "    print(f\"   Total execution time: {execution_time:.4f} ms\")\n",
    "    \n",
    "    # CPU/GPU 시간 분석\n",
    "    cpu_time = sum(info['time_ms'] for info in safe_controller.execution_times.values() if info['device'] == 'CPU')\n",
    "    gpu_time = sum(info['time_ms'] for info in safe_controller.execution_times.values() if info['device'] == 'GPU')\n",
    "    \n",
    "    print(f\"   CPU layers time: {cpu_time:.4f} ms\")\n",
    "    print(f\"   GPU layers time: {gpu_time:.4f} ms\")\n",
    "    if gpu_time > 0:\n",
    "        print(f\"   CPU/GPU ratio: {cpu_time/gpu_time:.2f}\")\n",
    "    \n",
    "    # 상세 리포트 (처음 10개 레이어만)\n",
    "    print(f\"\\nFirst 10 layers timing:\")\n",
    "    count = 0\n",
    "    for layer_name, info in safe_controller.execution_times.items():\n",
    "        if count < 100:\n",
    "            print(f\"  {layer_name}: {info['time_ms']:.4f}ms ({info['device']})\")\n",
    "            count += 1\n",
    "    \n",
    "    # 정리\n",
    "    safe_controller.remove_hooks()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎉 SUCCESS! SafeLayerDeviceController working perfectly!\")\n",
    "print(\"✅ No more device mismatch errors\")\n",
    "print(\"✅ Both layers and data moved together\")\n",
    "print(\"✅ Timing measurements accurate\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 사용법 요약\n",
    "print(\"\\n📋 FINAL USAGE INSTRUCTIONS:\")\n",
    "print(\"1. device_config = [1,1,0,0,1]  # 0=CPU, 1=GPU\")\n",
    "print(\"2. controller = SafeLayerDeviceController(model, device_config)\")\n",
    "print(\"3. output = controller.forward(input_tensor)\")\n",
    "print(\"4. controller.print_execution_report()  # Optional: detailed report\")\n",
    "print(\"5. controller.remove_hooks()  # IMPORTANT: Always clean up!\")\n",
    "print(\"\\n🚀 Ready to use with any configuration you want!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "022f06a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogLeNet(\n",
       "  (conv1): BasicConv2d(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (conv2): BasicConv2d(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv3): BasicConv2d(\n",
       "    (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception3a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception3b): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception4a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception4b): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception4c): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception4d): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception4e): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (inception5a): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (inception5b): Inception(\n",
       "    (branch1): BasicConv2d(\n",
       "      (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch2): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch3): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (branch4): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (aux1): None\n",
       "  (aux2): None\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GoogLeNet 모델 불러오기 (pretrained weights 사용 가능)\n",
    "import torchvision.models as models\n",
    "\n",
    "model_googlenet = models.googlenet(pretrained=True)\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model_googlenet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34e4086e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTING SafeLayerDeviceController\n",
      "============================================================\n",
      "Test 1: Simple mixed configuration\n",
      "----------------------------------------\n",
      "✅ Clean leaf layers created: 130 layers\n",
      "Config: First 3 GPU, next 3 CPU, rest GPU\n",
      "Total layers: 130\n",
      "GPU layers: 127, CPU layers: 3\n",
      "Pattern (first 20): GGGCCCGGGGGGGGGGGGGG...\n",
      "✅ SafeLayerDeviceController created successfully!\n",
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "✅ SUCCESSFUL EXECUTION!\n",
      "   Output shape: torch.Size([1, 1000])\n",
      "   Output device: cuda:0\n",
      "   Total execution time: 167.6002 ms\n",
      "   CPU layers time: 5.9907 ms\n",
      "   GPU layers time: 137.9893 ms\n",
      "   CPU/GPU ratio: 0.04\n",
      "\n",
      "First 10 layers timing:\n",
      "  Layer_00_conv1.conv: 1.7931ms (GPU)\n",
      "  Layer_01_conv1.bn: 5.4972ms (GPU)\n",
      "  Layer_02_maxpool1: 0.1106ms (GPU)\n",
      "  Layer_03_conv2.conv: 2.4514ms (CPU)\n",
      "  Layer_04_conv2.bn: 0.2651ms (CPU)\n",
      "  Layer_05_conv3.conv: 3.2742ms (CPU)\n",
      "  Layer_06_conv3.bn: 1.9667ms (GPU)\n",
      "  Layer_07_maxpool2: 0.2058ms (GPU)\n",
      "  Layer_08_inception3a.branch1.conv: 7.0903ms (GPU)\n",
      "  Layer_09_inception3a.branch1.bn: 0.6003ms (GPU)\n",
      "  Layer_10_inception3a.branch2.0.conv: 3.8366ms (GPU)\n",
      "  Layer_11_inception3a.branch2.0.bn: 0.4268ms (GPU)\n",
      "  Layer_12_inception3a.branch2.1.conv: 5.0130ms (GPU)\n",
      "  Layer_13_inception3a.branch2.1.bn: 0.4263ms (GPU)\n",
      "  Layer_14_inception3a.branch3.0.conv: 2.4900ms (GPU)\n",
      "  Layer_15_inception3a.branch3.0.bn: 0.3200ms (GPU)\n",
      "  Layer_16_inception3a.branch3.1.conv: 2.0993ms (GPU)\n",
      "  Layer_17_inception3a.branch3.1.bn: 0.3083ms (GPU)\n",
      "  Layer_18_inception3a.branch4.0: 0.0894ms (GPU)\n",
      "  Layer_19_inception3a.branch4.1.conv: 2.1300ms (GPU)\n",
      "  Layer_20_inception3a.branch4.1.bn: 0.3054ms (GPU)\n",
      "  Layer_21_inception3b.branch1.conv: 2.0318ms (GPU)\n",
      "  Layer_22_inception3b.branch1.bn: 0.3073ms (GPU)\n",
      "  Layer_23_inception3b.branch2.0.conv: 0.3796ms (GPU)\n",
      "  Layer_24_inception3b.branch2.0.bn: 0.2756ms (GPU)\n",
      "  Layer_25_inception3b.branch2.1.conv: 2.7280ms (GPU)\n",
      "  Layer_26_inception3b.branch2.1.bn: 0.4096ms (GPU)\n",
      "  Layer_27_inception3b.branch3.0.conv: 1.5688ms (GPU)\n",
      "  Layer_28_inception3b.branch3.0.bn: 0.2363ms (GPU)\n",
      "  Layer_29_inception3b.branch3.1.conv: 1.6713ms (GPU)\n",
      "  Layer_30_inception3b.branch3.1.bn: 0.2220ms (GPU)\n",
      "  Layer_31_inception3b.branch4.0: 0.0646ms (GPU)\n",
      "  Layer_32_inception3b.branch4.1.conv: 1.5476ms (GPU)\n",
      "  Layer_33_inception3b.branch4.1.bn: 0.2446ms (GPU)\n",
      "  Layer_34_maxpool3: 0.0637ms (GPU)\n",
      "  Layer_35_inception4a.branch1.conv: 1.7397ms (GPU)\n",
      "  Layer_36_inception4a.branch1.bn: 0.2227ms (GPU)\n",
      "  Layer_37_inception4a.branch2.0.conv: 1.5123ms (GPU)\n",
      "  Layer_38_inception4a.branch2.0.bn: 0.2344ms (GPU)\n",
      "  Layer_39_inception4a.branch2.1.conv: 2.2984ms (GPU)\n",
      "  Layer_40_inception4a.branch2.1.bn: 0.2229ms (GPU)\n",
      "  Layer_41_inception4a.branch3.0.conv: 1.2703ms (GPU)\n",
      "  Layer_42_inception4a.branch3.0.bn: 0.2205ms (GPU)\n",
      "  Layer_43_inception4a.branch3.1.conv: 1.4744ms (GPU)\n",
      "  Layer_44_inception4a.branch3.1.bn: 0.2182ms (GPU)\n",
      "  Layer_45_inception4a.branch4.0: 0.0608ms (GPU)\n",
      "  Layer_46_inception4a.branch4.1.conv: 1.6489ms (GPU)\n",
      "  Layer_47_inception4a.branch4.1.bn: 0.2768ms (GPU)\n",
      "  Layer_48_inception4b.branch1.conv: 1.6544ms (GPU)\n",
      "  Layer_49_inception4b.branch1.bn: 0.2177ms (GPU)\n",
      "  Layer_50_inception4b.branch2.0.conv: 1.5316ms (GPU)\n",
      "  Layer_51_inception4b.branch2.0.bn: 0.2224ms (GPU)\n",
      "  Layer_52_inception4b.branch2.1.conv: 2.6457ms (GPU)\n",
      "  Layer_53_inception4b.branch2.1.bn: 0.2344ms (GPU)\n",
      "  Layer_54_inception4b.branch3.0.conv: 1.2436ms (GPU)\n",
      "  Layer_55_inception4b.branch3.0.bn: 0.2131ms (GPU)\n",
      "  Layer_56_inception4b.branch3.1.conv: 1.5211ms (GPU)\n",
      "  Layer_57_inception4b.branch3.1.bn: 0.2403ms (GPU)\n",
      "  Layer_58_inception4b.branch4.0: 0.0639ms (GPU)\n",
      "  Layer_59_inception4b.branch4.1.conv: 1.3788ms (GPU)\n",
      "  Layer_60_inception4b.branch4.1.bn: 0.2174ms (GPU)\n",
      "  Layer_61_inception4c.branch1.conv: 1.5171ms (GPU)\n",
      "  Layer_62_inception4c.branch1.bn: 0.2127ms (GPU)\n",
      "  Layer_63_inception4c.branch2.0.conv: 0.4776ms (GPU)\n",
      "  Layer_64_inception4c.branch2.0.bn: 0.1979ms (GPU)\n",
      "  Layer_65_inception4c.branch2.1.conv: 2.7339ms (GPU)\n",
      "  Layer_66_inception4c.branch2.1.bn: 0.2165ms (GPU)\n",
      "  Layer_67_inception4c.branch3.0.conv: 0.2012ms (GPU)\n",
      "  Layer_68_inception4c.branch3.0.bn: 0.1984ms (GPU)\n",
      "  Layer_69_inception4c.branch3.1.conv: 0.2081ms (GPU)\n",
      "  Layer_70_inception4c.branch3.1.bn: 0.1850ms (GPU)\n",
      "  Layer_71_inception4c.branch4.0: 0.2277ms (GPU)\n",
      "  Layer_72_inception4c.branch4.1.conv: 0.3109ms (GPU)\n",
      "  Layer_73_inception4c.branch4.1.bn: 0.1893ms (GPU)\n",
      "  Layer_74_inception4d.branch1.conv: 0.4065ms (GPU)\n",
      "  Layer_75_inception4d.branch1.bn: 0.1903ms (GPU)\n",
      "  Layer_76_inception4d.branch2.0.conv: 1.5206ms (GPU)\n",
      "  Layer_77_inception4d.branch2.0.bn: 0.2105ms (GPU)\n",
      "  Layer_78_inception4d.branch2.1.conv: 3.0639ms (GPU)\n",
      "  Layer_79_inception4d.branch2.1.bn: 0.2110ms (GPU)\n",
      "  Layer_80_inception4d.branch3.0.conv: 1.2648ms (GPU)\n",
      "  Layer_81_inception4d.branch3.0.bn: 0.2129ms (GPU)\n",
      "  Layer_82_inception4d.branch3.1.conv: 1.5564ms (GPU)\n",
      "  Layer_83_inception4d.branch3.1.bn: 0.2091ms (GPU)\n",
      "  Layer_84_inception4d.branch4.0: 0.0584ms (GPU)\n",
      "  Layer_85_inception4d.branch4.1.conv: 0.3006ms (GPU)\n",
      "  Layer_86_inception4d.branch4.1.bn: 0.1855ms (GPU)\n",
      "  Layer_87_inception4e.branch1.conv: 1.7769ms (GPU)\n",
      "  Layer_88_inception4e.branch1.bn: 0.2124ms (GPU)\n",
      "  Layer_89_inception4e.branch2.0.conv: 1.6916ms (GPU)\n",
      "  Layer_90_inception4e.branch2.0.bn: 0.2060ms (GPU)\n",
      "  Layer_91_inception4e.branch2.1.conv: 3.4122ms (GPU)\n",
      "  Layer_92_inception4e.branch2.1.bn: 0.2365ms (GPU)\n",
      "  Layer_93_inception4e.branch3.0.conv: 1.3051ms (GPU)\n",
      "  Layer_94_inception4e.branch3.0.bn: 0.2077ms (GPU)\n",
      "  Layer_95_inception4e.branch3.1.conv: 1.5707ms (GPU)\n",
      "  Layer_96_inception4e.branch3.1.bn: 0.2060ms (GPU)\n",
      "  Layer_97_inception4e.branch4.0: 0.0572ms (GPU)\n",
      "  Layer_98_inception4e.branch4.1.conv: 1.4942ms (GPU)\n",
      "  Layer_99_inception4e.branch4.1.bn: 0.2134ms (GPU)\n",
      "Restoring model to original state...\n",
      "Model restored to original devices\n",
      "All hooks removed and model restored!\n",
      "\n",
      "============================================================\n",
      "🎉 SUCCESS! SafeLayerDeviceController working perfectly!\n",
      "✅ No more device mismatch errors\n",
      "✅ Both layers and data moved together\n",
      "✅ Timing measurements accurate\n",
      "============================================================\n",
      "\n",
      "📋 FINAL USAGE INSTRUCTIONS:\n",
      "1. device_config = [1,1,0,0,1]  # 0=CPU, 1=GPU\n",
      "2. controller = SafeLayerDeviceController(model, device_config)\n",
      "3. output = controller.forward(input_tensor)\n",
      "4. controller.print_execution_report()  # Optional: detailed report\n",
      "5. controller.remove_hooks()  # IMPORTANT: Always clean up!\n",
      "\n",
      "🚀 Ready to use with any configuration you want!\n"
     ]
    }
   ],
   "source": [
    "# 🧪 TESTING SafeLayerDeviceController - 에러 없는 안전한 버전\n",
    "print(\"🧪 TESTING SafeLayerDeviceController\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 첫 번째 테스트: 간단한 혼합 설정\n",
    "print(\"Test 1: Simple mixed configuration\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 처음 3개는 GPU, 다음 3개는 CPU, 나머지는 GPU\n",
    "leaf_layers = get_clean_leaf_layers(model_googlenet)\n",
    "num_layers = len(leaf_layers)\n",
    "print(f\"✅ Clean leaf layers created: {num_layers} layers\")\n",
    "test_config = [1] * 3 + [0] * 3 + [1] * (num_layers - 6)\n",
    "print(f\"Config: First 3 GPU, next 3 CPU, rest GPU\")\n",
    "print(f\"Total layers: {len(test_config)}\")\n",
    "print(f\"GPU layers: {sum(test_config)}, CPU layers: {len(test_config) - sum(test_config)}\")\n",
    "\n",
    "# 설정 시각화\n",
    "config_str = ''.join(['G' if x == 1 else 'C' for x in test_config[:20]])\n",
    "print(f\"Pattern (first 20): {config_str}...\")\n",
    "\n",
    "try:\n",
    "    # Controller 생성\n",
    "    safe_controller = SafeLayerDeviceController(model_googlenet, test_config)\n",
    "    print(\"✅ SafeLayerDeviceController created successfully!\")\n",
    "    \n",
    "    # 테스트 실행\n",
    "    test_input = torch.randn(1, 3, 224, 224)\n",
    "    print(f\"Input shape: {test_input.shape}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    output = safe_controller.forward(test_input)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = (end_time - start_time) * 1000\n",
    "    print(f\"✅ SUCCESSFUL EXECUTION!\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Output device: {output.device}\")\n",
    "    print(f\"   Total execution time: {execution_time:.4f} ms\")\n",
    "    \n",
    "    # CPU/GPU 시간 분석\n",
    "    cpu_time = sum(info['time_ms'] for info in safe_controller.execution_times.values() if info['device'] == 'CPU')\n",
    "    gpu_time = sum(info['time_ms'] for info in safe_controller.execution_times.values() if info['device'] == 'GPU')\n",
    "    \n",
    "    print(f\"   CPU layers time: {cpu_time:.4f} ms\")\n",
    "    print(f\"   GPU layers time: {gpu_time:.4f} ms\")\n",
    "    if gpu_time > 0:\n",
    "        print(f\"   CPU/GPU ratio: {cpu_time/gpu_time:.2f}\")\n",
    "    \n",
    "    # 상세 리포트 (처음 10개 레이어만)\n",
    "    print(f\"\\nFirst 10 layers timing:\")\n",
    "    count = 0\n",
    "    for layer_name, info in safe_controller.execution_times.items():\n",
    "        if count < 100:\n",
    "            print(f\"  {layer_name}: {info['time_ms']:.4f}ms ({info['device']})\")\n",
    "            count += 1\n",
    "    \n",
    "    # 정리\n",
    "    safe_controller.remove_hooks()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎉 SUCCESS! SafeLayerDeviceController working perfectly!\")\n",
    "print(\"✅ No more device mismatch errors\")\n",
    "print(\"✅ Both layers and data moved together\")\n",
    "print(\"✅ Timing measurements accurate\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 사용법 요약\n",
    "print(\"\\n📋 FINAL USAGE INSTRUCTIONS:\")\n",
    "print(\"1. device_config = [1,1,0,0,1]  # 0=CPU, 1=GPU\")\n",
    "print(\"2. controller = SafeLayerDeviceController(model, device_config)\")\n",
    "print(\"3. output = controller.forward(input_tensor)\")\n",
    "print(\"4. controller.print_execution_report()  # Optional: detailed report\")\n",
    "print(\"5. controller.remove_hooks()  # IMPORTANT: Always clean up!\")\n",
    "print(\"\\n🚀 Ready to use with any configuration you want!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72988005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.models import mnasnet1_0, Mnasnet1_0_Weights\n",
    "from torchvision.models import googlenet, GoogLeNet_Weights\n",
    "from torchvision.models import squeezenet1_0, SqueezeNet1_0_Weights\n",
    "\n",
    "weights = SqueezeNet1_0_Weights.DEFAULT\n",
    "model_squeezenet = squeezenet1_0(weights=weights)\n",
    "model_squeezenet.eval()\n",
    "\n",
    "weights = GoogLeNet_Weights.DEFAULT\n",
    "model_googlenet = googlenet(weights=weights)\n",
    "model_googlenet.eval()\n",
    "\n",
    "weights = MobileNet_V2_Weights.DEFAULT\n",
    "model_mobilenet = mobilenet_v2(weights=weights)\n",
    "model_mobilenet.eval()\n",
    "\n",
    "\n",
    "weights = Mnasnet1_0_Weights.DEFAULT\n",
    "model_mnasnet = mnasnet1_0(weights=weights)\n",
    "model_mnasnet.eval()\n",
    "\n",
    "model = {\"squeezenet\": model_squeezenet,\n",
    "         \"googlenet\": model_googlenet,\n",
    "         \"mobilenet\": model_mobilenet,\n",
    "         \"mnasnet\": model_mnasnet}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "good",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
