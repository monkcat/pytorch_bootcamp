{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd98103-8064-46e0-8561-8bb59ec82a4b",
   "metadata": {},
   "source": [
    "# Understanding Hooks \n",
    "\n",
    "[reference] <br>\n",
    "https://www.digitalocean.com/community/tutorials/pytorch-hooks-gradient-clipping-debugging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af670d-a7bb-46c5-84f5-0e318fc16e47",
   "metadata": {},
   "source": [
    "PyTorch Hooks는 모델 아키텍처를 변경하지 않고도 학습 과정을 디버깅, 활성화값(activation)을 시각화하거나 기울기를 수정할 수 있다. <br>\n",
    "Hooks는 내부 동작이 불투명한 딥러닝 모델의 내부 흐름을 관찰하고 해석하는데 큰 도움이 된다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc3dda-af3e-4835-918a-0c0e1e212e97",
   "metadata": {},
   "source": [
    "## PyTorch Hooks 소개\n",
    "\n",
    "Hooks가 PyTorch에서 중요한 역할을 하는 이유 중 하나는 역전파(backpropagation) 중에 모델과 상호작용할 수 있도록 해주기 때문에 중요하다. <br>\n",
    "예를 들어, 학습 중간에 특정 층의 출력값이나 기울기를 기록하거나 조작 할 수 있어 디버깅 및 모델 해석에 매우 효과적이다. <br>\n",
    "Hook은 단순히 하나의 함수이며, Tensor나 nn.Module에 연결하면 forward나 backward 과정 중에 자동으로 실행된다. <br>\n",
    "여기서 말하는 forward는 module 클래스 내에 forward() 매서드를 의미하는 것이 아니고 torch.autograd.Function 클래스 내부에서 이루어진다. <br>\n",
    "(이 부분은 PyTorch가 모든 연산을 자동으로 기록하여 역전파를 자동 계산할 수 있게 해주는 기반 구조이다.)\n",
    "\n",
    "PyTorch에서 연산을 통해 생성된 모든 Tensor에는 grad_fn이라는 속성이 존재한다. 이 grad_fn는 torch.autograd.Function의 인스턴스로 해당 텐서를 생성한 연산을 나타낸다. 이 덕분에 PyTorch는 어떤 연산이 어떻게 이루어졌는지 추적할 수 있고, 역전파 시 그 경로를 따라 기울기를 계산할 수 있다. <br>\n",
    "예를 들어 tensor = tensor1 + tensor2 를 계산하면 output인 tensor에는 AddBackward 타입의 grad_fn이 붙는다. 이는 해당 덧셈 연산을 통해 기울기를 어떻게 계산할지를 내부에서 알고 있다는 뜻이다. (이 부분이 이해되지 않는다면 PyTorch 계산 그래프에 관련된 글을 찾아보길 바란다.) <br>\n",
    "간단히 요약하자면, 연산을 통해 생성된 모든 텐서는 grad_fn을 통해 생성 경로를 추적할 수 있다는 점이다. \n",
    "\n",
    "여기서 중요한 사실은 nn.Linear 같은 nn.Module 객체는 내부적으로 여러 연산으로 구성되어 있다. Linear는 $ Y = W * X + B$ 와 같은 방정식으로 행렬 곱셈 후 덧셈 이라는 연산을 수행한다. autograd 수준에서는 곱셈과 덧셈 각각에 대해 별도의 forward 연산이 발생하는 거지, 하나의 forward() 함수 호출마 있는 것은 아니다. <br>\n",
    "이 점을 고려하지 않고 Hooks를 사용하면 전체 레이어가 아닌 개별 연산에 hook이 걸릴 수 있어, 여러 개의 출력이 생기거나 예상치 못한 동작이 발생할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5766f2-9e68-45e9-ad76-b30a7aceeb47",
   "metadata": {},
   "source": [
    "## Hooks 종류\n",
    "\n",
    "1. Forward Hook : 순전파 시 (모델이 입력을 받아 출력을 계산하는 과정) 실행된다. \n",
    "2. Backward Hook : 역전파 시 (기울기를 계산해 파라미터를 업데이트하는 과정) 실행된다.\n",
    "\n",
    "PyTorch는 모든 연산을 자동 미분할 수 있도록, 내부적으로 torch.autograd.Function을 사용해 forward/backward 동작을 추적한다. 이 구조에 hook도 연결되어 동작한다. \n",
    "\n",
    "- register_forward_hook() : 순전파 때 작동\n",
    "- register_full_backward_hook() : 역전파 때 작동\n",
    "(register_backward_hook()은 권장되지 않고 full을 사용할 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd9fe5-bc74-4921-a936-a8d6893be4ca",
   "metadata": {},
   "source": [
    "## Tensor에 대한 Hook\n",
    "\n",
    "Hook은 특정한 형식을 갖춘 함수이다. Hook이 실행된다는 것은 실제로는 해당 함수를 PyTorch가 자동으로 호출한다는 뜻이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd56f47-9e36-41cf-904c-8a8df2b6a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor에 대한 backward hook 함수의 형태는 다음과 같다. \n",
    "\n",
    "hook(grad) -> Tensor 또는 None\n",
    "\n",
    "## 설명\n",
    "# grad : backward가 호출된 후, 해당 Tensor의 .grad 속성에 저장되는 기울기 값\n",
    "# 이 함수는 grad를 변경하지 않고 반환하거나, 수정된 새로운 Tensor를 반환해야 한다. \n",
    "# 반환된 Tensor는 이후 역전파 계산에 원래의 기울기 대신 사용\n",
    "# None을 반환하면 기존의 grad 값이 그대로 사용된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67428082-7de1-4b0c-99ad-e4bf48d5c9b7",
   "metadata": {},
   "source": [
    "### Tensor는 forward hook을 지원하지 않는다. \n",
    "\n",
    "즉 forward hook 기능이 존재하지 않고, backward hook만 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06bac95f-b43a-4a7d-82a4-72dff8029516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) \n",
      " tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2 * a\n",
    "b.retain_grad() # b는 non-leaf 텐서이므로, .grad를 유지하려면 retain_grad()를 호출해야 한다.\n",
    "\n",
    "c = b.mean()\n",
    "c.backward()\n",
    "\n",
    "print(a.grad,\"\\n\",b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85933ede-04a1-4a96-9777-d3853ec53ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) \n",
      " tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2 * a\n",
    "b.retain_grad() \n",
    "\n",
    "def print_hook(x):\n",
    "    print(x)\n",
    "    return None\n",
    "\n",
    "b.register_hook(print_hook) # hook 함수 등록할 때는 그냥 함수 자체를 괄호에 넣어야 함\n",
    "b.mean().backward()\n",
    "\n",
    "print(a.grad,\"\\n\",b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a055a6d-76cf-4e67-a38b-dc6af85ab930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000]) tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a.requires_grad = True\n",
    "\n",
    "b = 2*a\n",
    "b.retain_grad()\n",
    "\n",
    "b.mean().backward() \n",
    "\n",
    "print(a.grad, b.grad)\n",
    "\n",
    "b.grad *= 2\n",
    "\n",
    "print(a.grad, b.grad)\n",
    "\n",
    "# b.grad *= 2는 b의 기울기를 변경했지만, a의 기울기는 이미 계산이 끝난 뒤라 바뀌지 않음\n",
    "# 즉, 역전파 중간에 b.grad를 수정하고 싶다면, hook을 써야 함\n",
    "# 그렇지 않으면 b에 의존하는 모든 텐서의 grad를 수동으로 수정해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab00b15-260f-42cd-a859-cf15ce432ba5",
   "metadata": {},
   "source": [
    "## nn.Module 객체에 대한 Hook\n",
    "\n",
    "nn.Module 객체에 대해 hook 함수의 형식(signature)는 다음과 같다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de0392-e45e-4068-b647-b5fab25421b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward hook:\n",
    "\n",
    "hook(module, grad_input, grad_output) -> Tensor 또는 None\n",
    "\n",
    "# Forward hook:\n",
    "hook(module, input, output) -> None\n",
    "\n",
    "## 설명\n",
    "# module : hook이 등록된 nn.Module 객체 자체\n",
    "# input/grad_input : 해당 모듈의 입력 값 또는 입력에 대한 기울기\n",
    "# output / grad_output : 해당 모듈의 출력 값 또는 출력에 대한 기울기\n",
    "# forward hook은 출력값을 관찰하는 데 유용하고, backward hook은 기울기를 추적하거나 조정하는 데 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfc1dc-15b6-4e32-95be-5c50f92a7881",
   "metadata": {},
   "source": [
    "## 왜 Module 객체에 hook 을 조심히 사용해야 하는가\n",
    "\n",
    "hook을 사용하기 위해서는 module 내부의 추상화를 깨야 한다.<br>\n",
    "nn.Module은 일반적으로 하나의 레이어를 나타내는 모듈화된 객체이지만 실제로는 여러 연산을 수행할 수 있다. 즉 하나의 모듈에 대해 여러 번의 forward나 backward가 일어날 수 있으며, 이 구조를 이해하지 못하면 hook의 위치와 의미를 혼동하게 된다. 즉 내부적으로 Add나 MatMul 같은 여러 연산이 묶여 있기 때문에 hook이 어디에 걸렸는지 명확히 인식해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e4e17-a262-42ab-b36c-eb51d87e1020",
   "metadata": {},
   "source": [
    "## 예) nn.Linear는 내부적으로 어떻게 구성되어 있을까?\n",
    "\n",
    "Linear 레이어는 내부적으로 행렬 곱과 덧셈 두가지 연산을 수행한다. $Y = W * X + b$ <br>\n",
    "이 두 연산은 별도의 노드로 인식한다. 따라서 이 레이어에 forward hook을 등록하면 input이 Tensor가 아닌 **tuple 형태**일 수 있다. -> 이는 각 개별 연산의 입력을 의미하기 때문이다. <br>\n",
    "마찬가지로, output도 단일 값이 아닌, 연산 시점의 특정 출력값일 수 있다. \n",
    "\n",
    "> Hook을 모듈 수준에 걸면 전체 레이어의 입출력이나 기울기를 다루는 게 아니라, <br> 그 시점의 연산 흐름 중 일부를 다룬다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c633d655-eac6-400e-9939-d0c1722bfaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=160, out_features=5, bias=True)\n",
      "--------------Input Grad---------------\n",
      "torch.Size([1, 5])\n",
      "--------------Output Grad---------------\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "Conv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))\n",
      "--------------Input Grad---------------\n",
      "None found for Gradient\n",
      "torch.Size([10, 3, 2, 2])\n",
      "torch.Size([10])\n",
      "--------------Output Grad---------------\n",
      "torch.Size([1, 10, 4, 4])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 10, 2, stride = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.flatten = lambda x: x.view(-1)\n",
    "        self.fc1 = nn.Linear(160, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv(x))\n",
    "        return self.fc1(self.flatten(x))\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "    print(m)\n",
    "    print(\"--------------Input Grad---------------\")\n",
    "\n",
    "    for grad in i:\n",
    "        try:\n",
    "            print(grad.shape)\n",
    "        except AttributeError:\n",
    "            print(\"None found for Gradient\")\n",
    "    \n",
    "    print(\"--------------Output Grad---------------\")\n",
    "    for grad in o:\n",
    "        try:\n",
    "            print(grad.shape)\n",
    "        except AttributeError:\n",
    "            print(\"None found for Gradient\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "net = myNet()\n",
    "\n",
    "net.conv.register_backward_hook(hook_fn)\n",
    "net.fc1.register_backward_hook(hook_fn)\n",
    "\n",
    "inp = torch.randn(1, 3, 8, 8)\n",
    "out = net(inp)\n",
    "(1 - out.mean()).backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b530ba4-2443-4dd0-b4bb-d00a9ba290c0",
   "metadata": {},
   "source": [
    "### Conv2d의 Grad 해석\n",
    "- grad_input:\n",
    "    -  [10, 3, 2, 2]: weight의 기울기\n",
    "    -  [10]: bias의 기울기\n",
    "    -  None : 입력 feature map의 기울기 (conv 앞 레이어에서 받아야 하므로 이 시점에는 없음)\n",
    "  Conv는 내부적으로 im2col 같은 방법으로 이미지 데이터를 펼쳐서 행렬곱 방식으로 계산함<br>\n",
    "    -> 이로 인해 연산이 나뉘고 hook이 예상과 다르게 동작할 수 있음\n",
    "\n",
    "### Linear의 Grad 해석\n",
    "- grad_input 이 둘 다 [5] : 이게 왜이럴까?\n",
    "    - 사실 fc1의 weight는 [160, 5]인데 왜 [5] 일까?\n",
    "    - 내부 동작을 정확하게 이해하지 않으면 기울기 방향과 구조가 직관과 다를 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ca3e1-8276-49e9-87b7-cb8bfc414790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "good",
   "language": "python",
   "name": "good"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
